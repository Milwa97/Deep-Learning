{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(data, labels, lengths):\n",
    "    r\"\"\"\n",
    "    Randomly split a dataset into non-overlapping new datasets of given lengths.\n",
    "\n",
    "    Arguments:  ## TO DO ##\n",
    "        data (): Dataset to be split\n",
    "        labels (): labels\n",
    "        lengths (sequence): lengths of splits to be produced\n",
    "    \"\"\"\n",
    "    if sum(lengths) != len(labels):\n",
    "        raise ValueError(\"Sum of input lengths does not equal the length of the input dataset!\")\n",
    "        \n",
    "    indices = torch.randperm(sum(lengths))\n",
    "    train_ind, test_ind = indices.split_with_sizes( lengths)\n",
    "        \n",
    "    train_data = data[train_ind]\n",
    "    test_data = data[test_ind]\n",
    "\n",
    "    train_labels = labels[train_ind]\n",
    "    test_labels = labels[test_ind]\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(train_data, train_labels)\n",
    "    test_dataset = torch.utils.data.TensorDataset(test_data, test_labels)   \n",
    "    \n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = np.load('./data/classification_problem.npy')\n",
    "data = torch.from_numpy(raw_data)\n",
    "labels = torch.from_numpy(np.array([0]*1000+[1]*1000+[2]*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, validation_dataset = random_split(data, labels, (2000,1000) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Y're fuckin' idiot. LEARN!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                           batch_size=50, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    nn.Linear(12,200), nn.ReLU(),\n",
    "    nn.BatchNorm1d(200),\n",
    "    nn.Linear(200,200), nn.ReLU(),\n",
    "    nn.BatchNorm1d(200),\n",
    "    nn.Linear(200,200), nn.ReLU(),\n",
    "    nn.BatchNorm1d(200),\n",
    "    nn.Linear(200,3),\n",
    "    nn.BatchNorm1d(3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=0.01, momentum= 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_f = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "batches = 0\n",
    "epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 loss:\t tensor(0.9672, grad_fn=<NllLossBackward>)\n",
      "epoch:  10 loss:\t tensor(0.7635, grad_fn=<NllLossBackward>)\n",
      "epoch:  20 loss:\t tensor(1.3630, grad_fn=<NllLossBackward>)\n",
      "epoch:  30 loss:\t tensor(0.5435, grad_fn=<NllLossBackward>)\n",
      "epoch:  40 loss:\t tensor(0.9031, grad_fn=<NllLossBackward>)\n",
      "epoch:  50 loss:\t tensor(0.5956, grad_fn=<NllLossBackward>)\n",
      "epoch:  60 loss:\t tensor(1.3046, grad_fn=<NllLossBackward>)\n",
      "epoch:  70 loss:\t tensor(0.7683, grad_fn=<NllLossBackward>)\n",
      "epoch:  80 loss:\t tensor(0.8874, grad_fn=<NllLossBackward>)\n",
      "epoch:  90 loss:\t tensor(0.9181, grad_fn=<NllLossBackward>)\n",
      "epoch:  100 loss:\t tensor(0.5822, grad_fn=<NllLossBackward>)\n",
      "epoch:  110 loss:\t tensor(0.7601, grad_fn=<NllLossBackward>)\n",
      "epoch:  120 loss:\t tensor(0.8753, grad_fn=<NllLossBackward>)\n",
      "epoch:  130 loss:\t tensor(0.5728, grad_fn=<NllLossBackward>)\n",
      "epoch:  140 loss:\t tensor(0.7023, grad_fn=<NllLossBackward>)\n",
      "epoch:  150 loss:\t tensor(1.5440, grad_fn=<NllLossBackward>)\n",
      "epoch:  160 loss:\t tensor(0.7460, grad_fn=<NllLossBackward>)\n",
      "epoch:  170 loss:\t tensor(0.6144, grad_fn=<NllLossBackward>)\n",
      "epoch:  180 loss:\t tensor(1.5557, grad_fn=<NllLossBackward>)\n",
      "epoch:  190 loss:\t tensor(0.5861, grad_fn=<NllLossBackward>)\n",
      "epoch:  200 loss:\t tensor(0.3849, grad_fn=<NllLossBackward>)\n",
      "epoch:  210 loss:\t tensor(0.7387, grad_fn=<NllLossBackward>)\n",
      "epoch:  220 loss:\t tensor(0.7117, grad_fn=<NllLossBackward>)\n",
      "epoch:  230 loss:\t tensor(0.7757, grad_fn=<NllLossBackward>)\n",
      "epoch:  240 loss:\t tensor(0.7774, grad_fn=<NllLossBackward>)\n",
      "epoch:  250 loss:\t tensor(0.9119, grad_fn=<NllLossBackward>)\n",
      "epoch:  260 loss:\t tensor(0.8870, grad_fn=<NllLossBackward>)\n",
      "epoch:  270 loss:\t tensor(0.8838, grad_fn=<NllLossBackward>)\n",
      "epoch:  280 loss:\t tensor(0.6785, grad_fn=<NllLossBackward>)\n",
      "epoch:  290 loss:\t tensor(0.4336, grad_fn=<NllLossBackward>)\n",
      "epoch:  300 loss:\t tensor(0.7584, grad_fn=<NllLossBackward>)\n",
      "epoch:  310 loss:\t tensor(0.5979, grad_fn=<NllLossBackward>)\n",
      "epoch:  320 loss:\t tensor(0.7014, grad_fn=<NllLossBackward>)\n",
      "epoch:  330 loss:\t tensor(1.0779, grad_fn=<NllLossBackward>)\n",
      "epoch:  340 loss:\t tensor(0.5567, grad_fn=<NllLossBackward>)\n",
      "epoch:  350 loss:\t tensor(0.7467, grad_fn=<NllLossBackward>)\n",
      "epoch:  360 loss:\t tensor(0.4059, grad_fn=<NllLossBackward>)\n",
      "epoch:  370 loss:\t tensor(0.7877, grad_fn=<NllLossBackward>)\n",
      "epoch:  380 loss:\t tensor(0.7615, grad_fn=<NllLossBackward>)\n",
      "epoch:  390 loss:\t tensor(0.5986, grad_fn=<NllLossBackward>)\n",
      "epoch:  400 loss:\t tensor(0.6500, grad_fn=<NllLossBackward>)\n",
      "epoch:  410 loss:\t tensor(0.8811, grad_fn=<NllLossBackward>)\n",
      "epoch:  420 loss:\t tensor(0.5726, grad_fn=<NllLossBackward>)\n",
      "epoch:  430 loss:\t tensor(0.4985, grad_fn=<NllLossBackward>)\n",
      "epoch:  440 loss:\t tensor(0.9259, grad_fn=<NllLossBackward>)\n",
      "epoch:  450 loss:\t tensor(0.6446, grad_fn=<NllLossBackward>)\n",
      "epoch:  460 loss:\t tensor(0.7351, grad_fn=<NllLossBackward>)\n",
      "epoch:  470 loss:\t tensor(0.5873, grad_fn=<NllLossBackward>)\n",
      "epoch:  480 loss:\t tensor(0.8392, grad_fn=<NllLossBackward>)\n",
      "epoch:  490 loss:\t tensor(0.4712, grad_fn=<NllLossBackward>)\n",
      "epoch:  500 loss:\t tensor(1.1922, grad_fn=<NllLossBackward>)\n",
      "epoch:  510 loss:\t tensor(0.6634, grad_fn=<NllLossBackward>)\n",
      "epoch:  520 loss:\t tensor(0.7142, grad_fn=<NllLossBackward>)\n",
      "epoch:  530 loss:\t tensor(1.0298, grad_fn=<NllLossBackward>)\n",
      "epoch:  540 loss:\t tensor(0.8418, grad_fn=<NllLossBackward>)\n",
      "epoch:  550 loss:\t tensor(0.4135, grad_fn=<NllLossBackward>)\n",
      "epoch:  560 loss:\t tensor(0.7463, grad_fn=<NllLossBackward>)\n",
      "epoch:  570 loss:\t tensor(0.5753, grad_fn=<NllLossBackward>)\n",
      "epoch:  580 loss:\t tensor(0.5236, grad_fn=<NllLossBackward>)\n",
      "epoch:  590 loss:\t tensor(0.6174, grad_fn=<NllLossBackward>)\n",
      "epoch:  600 loss:\t tensor(1.1428, grad_fn=<NllLossBackward>)\n",
      "epoch:  610 loss:\t tensor(1.4053, grad_fn=<NllLossBackward>)\n",
      "epoch:  620 loss:\t tensor(0.6550, grad_fn=<NllLossBackward>)\n",
      "epoch:  630 loss:\t tensor(0.7709, grad_fn=<NllLossBackward>)\n",
      "epoch:  640 loss:\t tensor(0.7344, grad_fn=<NllLossBackward>)\n",
      "epoch:  650 loss:\t tensor(0.5522, grad_fn=<NllLossBackward>)\n",
      "epoch:  660 loss:\t tensor(0.4781, grad_fn=<NllLossBackward>)\n",
      "epoch:  670 loss:\t tensor(0.7820, grad_fn=<NllLossBackward>)\n",
      "epoch:  680 loss:\t tensor(0.6595, grad_fn=<NllLossBackward>)\n",
      "epoch:  690 loss:\t tensor(1.2032, grad_fn=<NllLossBackward>)\n",
      "epoch:  700 loss:\t tensor(0.9480, grad_fn=<NllLossBackward>)\n",
      "epoch:  710 loss:\t tensor(0.3832, grad_fn=<NllLossBackward>)\n",
      "epoch:  720 loss:\t tensor(0.5073, grad_fn=<NllLossBackward>)\n",
      "epoch:  730 loss:\t tensor(0.8388, grad_fn=<NllLossBackward>)\n",
      "epoch:  740 loss:\t tensor(0.4786, grad_fn=<NllLossBackward>)\n",
      "epoch:  750 loss:\t tensor(0.6261, grad_fn=<NllLossBackward>)\n",
      "epoch:  760 loss:\t tensor(1.2271, grad_fn=<NllLossBackward>)\n",
      "epoch:  770 loss:\t tensor(1.0918, grad_fn=<NllLossBackward>)\n",
      "epoch:  780 loss:\t tensor(0.7308, grad_fn=<NllLossBackward>)\n",
      "epoch:  790 loss:\t tensor(1.0179, grad_fn=<NllLossBackward>)\n",
      "epoch:  800 loss:\t tensor(0.3763, grad_fn=<NllLossBackward>)\n",
      "epoch:  810 loss:\t tensor(0.7405, grad_fn=<NllLossBackward>)\n",
      "epoch:  820 loss:\t tensor(0.5441, grad_fn=<NllLossBackward>)\n",
      "epoch:  830 loss:\t tensor(0.5678, grad_fn=<NllLossBackward>)\n",
      "epoch:  840 loss:\t tensor(1.0230, grad_fn=<NllLossBackward>)\n",
      "epoch:  850 loss:\t tensor(1.0987, grad_fn=<NllLossBackward>)\n",
      "epoch:  860 loss:\t tensor(0.9057, grad_fn=<NllLossBackward>)\n",
      "epoch:  870 loss:\t tensor(0.7151, grad_fn=<NllLossBackward>)\n",
      "epoch:  880 loss:\t tensor(0.9299, grad_fn=<NllLossBackward>)\n",
      "epoch:  890 loss:\t tensor(0.6752, grad_fn=<NllLossBackward>)\n",
      "epoch:  900 loss:\t tensor(0.5445, grad_fn=<NllLossBackward>)\n",
      "epoch:  910 loss:\t tensor(0.7110, grad_fn=<NllLossBackward>)\n",
      "epoch:  920 loss:\t tensor(1.0908, grad_fn=<NllLossBackward>)\n",
      "epoch:  930 loss:\t tensor(0.7252, grad_fn=<NllLossBackward>)\n",
      "epoch:  940 loss:\t tensor(0.6968, grad_fn=<NllLossBackward>)\n",
      "epoch:  950 loss:\t tensor(0.8841, grad_fn=<NllLossBackward>)\n",
      "epoch:  960 loss:\t tensor(0.5760, grad_fn=<NllLossBackward>)\n",
      "epoch:  970 loss:\t tensor(0.7594, grad_fn=<NllLossBackward>)\n",
      "epoch:  980 loss:\t tensor(0.6246, grad_fn=<NllLossBackward>)\n",
      "epoch:  990 loss:\t tensor(0.9385, grad_fn=<NllLossBackward>)\n",
      "epoch:  1000 loss:\t tensor(0.6224, grad_fn=<NllLossBackward>)\n",
      "epoch:  1010 loss:\t tensor(0.7538, grad_fn=<NllLossBackward>)\n",
      "epoch:  1020 loss:\t tensor(0.8952, grad_fn=<NllLossBackward>)\n",
      "epoch:  1030 loss:\t tensor(0.7477, grad_fn=<NllLossBackward>)\n",
      "epoch:  1040 loss:\t tensor(0.9489, grad_fn=<NllLossBackward>)\n",
      "epoch:  1050 loss:\t tensor(0.7051, grad_fn=<NllLossBackward>)\n",
      "epoch:  1060 loss:\t tensor(1.0981, grad_fn=<NllLossBackward>)\n",
      "epoch:  1070 loss:\t tensor(1.2870, grad_fn=<NllLossBackward>)\n",
      "epoch:  1080 loss:\t tensor(0.7516, grad_fn=<NllLossBackward>)\n",
      "epoch:  1090 loss:\t tensor(0.9921, grad_fn=<NllLossBackward>)\n",
      "epoch:  1100 loss:\t tensor(1.0638, grad_fn=<NllLossBackward>)\n",
      "epoch:  1110 loss:\t tensor(1.0137, grad_fn=<NllLossBackward>)\n",
      "epoch:  1120 loss:\t tensor(0.7325, grad_fn=<NllLossBackward>)\n",
      "epoch:  1130 loss:\t tensor(0.8695, grad_fn=<NllLossBackward>)\n",
      "epoch:  1140 loss:\t tensor(0.6675, grad_fn=<NllLossBackward>)\n",
      "epoch:  1150 loss:\t tensor(1.0498, grad_fn=<NllLossBackward>)\n",
      "epoch:  1160 loss:\t tensor(1.1564, grad_fn=<NllLossBackward>)\n",
      "epoch:  1170 loss:\t tensor(0.9800, grad_fn=<NllLossBackward>)\n",
      "epoch:  1180 loss:\t tensor(0.5092, grad_fn=<NllLossBackward>)\n",
      "epoch:  1190 loss:\t tensor(0.8041, grad_fn=<NllLossBackward>)\n",
      "epoch:  1200 loss:\t tensor(1.2089, grad_fn=<NllLossBackward>)\n",
      "epoch:  1210 loss:\t tensor(0.4965, grad_fn=<NllLossBackward>)\n",
      "epoch:  1220 loss:\t tensor(0.6541, grad_fn=<NllLossBackward>)\n",
      "epoch:  1230 loss:\t tensor(0.6502, grad_fn=<NllLossBackward>)\n",
      "epoch:  1240 loss:\t tensor(0.9148, grad_fn=<NllLossBackward>)\n",
      "epoch:  1250 loss:\t tensor(1.1142, grad_fn=<NllLossBackward>)\n",
      "epoch:  1260 loss:\t tensor(0.5266, grad_fn=<NllLossBackward>)\n",
      "epoch:  1270 loss:\t tensor(0.6488, grad_fn=<NllLossBackward>)\n",
      "epoch:  1280 loss:\t tensor(0.5952, grad_fn=<NllLossBackward>)\n",
      "epoch:  1290 loss:\t tensor(1.1172, grad_fn=<NllLossBackward>)\n",
      "epoch:  1300 loss:\t tensor(1.1708, grad_fn=<NllLossBackward>)\n",
      "epoch:  1310 loss:\t tensor(0.8573, grad_fn=<NllLossBackward>)\n",
      "epoch:  1320 loss:\t tensor(0.6616, grad_fn=<NllLossBackward>)\n",
      "epoch:  1330 loss:\t tensor(0.6132, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1340 loss:\t tensor(0.6140, grad_fn=<NllLossBackward>)\n",
      "epoch:  1350 loss:\t tensor(0.9024, grad_fn=<NllLossBackward>)\n",
      "epoch:  1360 loss:\t tensor(1.0336, grad_fn=<NllLossBackward>)\n",
      "epoch:  1370 loss:\t tensor(0.5501, grad_fn=<NllLossBackward>)\n",
      "epoch:  1380 loss:\t tensor(0.9797, grad_fn=<NllLossBackward>)\n",
      "epoch:  1390 loss:\t tensor(0.9687, grad_fn=<NllLossBackward>)\n",
      "epoch:  1400 loss:\t tensor(0.9364, grad_fn=<NllLossBackward>)\n",
      "epoch:  1410 loss:\t tensor(0.6941, grad_fn=<NllLossBackward>)\n",
      "epoch:  1420 loss:\t tensor(0.8235, grad_fn=<NllLossBackward>)\n",
      "epoch:  1430 loss:\t tensor(0.6256, grad_fn=<NllLossBackward>)\n",
      "epoch:  1440 loss:\t tensor(0.6011, grad_fn=<NllLossBackward>)\n",
      "epoch:  1450 loss:\t tensor(0.5100, grad_fn=<NllLossBackward>)\n",
      "epoch:  1460 loss:\t tensor(1.2216, grad_fn=<NllLossBackward>)\n",
      "epoch:  1470 loss:\t tensor(0.9274, grad_fn=<NllLossBackward>)\n",
      "epoch:  1480 loss:\t tensor(0.5997, grad_fn=<NllLossBackward>)\n",
      "epoch:  1490 loss:\t tensor(1.0507, grad_fn=<NllLossBackward>)\n",
      "epoch:  1500 loss:\t tensor(0.6793, grad_fn=<NllLossBackward>)\n",
      "epoch:  1510 loss:\t tensor(1.1058, grad_fn=<NllLossBackward>)\n",
      "epoch:  1520 loss:\t tensor(0.6213, grad_fn=<NllLossBackward>)\n",
      "epoch:  1530 loss:\t tensor(0.4428, grad_fn=<NllLossBackward>)\n",
      "epoch:  1540 loss:\t tensor(0.5717, grad_fn=<NllLossBackward>)\n",
      "epoch:  1550 loss:\t tensor(0.4848, grad_fn=<NllLossBackward>)\n",
      "epoch:  1560 loss:\t tensor(0.5669, grad_fn=<NllLossBackward>)\n",
      "epoch:  1570 loss:\t tensor(0.9542, grad_fn=<NllLossBackward>)\n",
      "epoch:  1580 loss:\t tensor(0.5640, grad_fn=<NllLossBackward>)\n",
      "epoch:  1590 loss:\t tensor(0.7834, grad_fn=<NllLossBackward>)\n",
      "epoch:  1600 loss:\t tensor(0.9227, grad_fn=<NllLossBackward>)\n",
      "epoch:  1610 loss:\t tensor(0.7902, grad_fn=<NllLossBackward>)\n",
      "epoch:  1620 loss:\t tensor(1.0122, grad_fn=<NllLossBackward>)\n",
      "epoch:  1630 loss:\t tensor(0.7456, grad_fn=<NllLossBackward>)\n",
      "epoch:  1640 loss:\t tensor(0.7011, grad_fn=<NllLossBackward>)\n",
      "epoch:  1650 loss:\t tensor(0.6730, grad_fn=<NllLossBackward>)\n",
      "epoch:  1660 loss:\t tensor(0.6073, grad_fn=<NllLossBackward>)\n",
      "epoch:  1670 loss:\t tensor(0.5165, grad_fn=<NllLossBackward>)\n",
      "epoch:  1680 loss:\t tensor(1.3537, grad_fn=<NllLossBackward>)\n",
      "epoch:  1690 loss:\t tensor(0.5970, grad_fn=<NllLossBackward>)\n",
      "epoch:  1700 loss:\t tensor(0.6701, grad_fn=<NllLossBackward>)\n",
      "epoch:  1710 loss:\t tensor(1.1071, grad_fn=<NllLossBackward>)\n",
      "epoch:  1720 loss:\t tensor(0.8101, grad_fn=<NllLossBackward>)\n",
      "epoch:  1730 loss:\t tensor(0.4549, grad_fn=<NllLossBackward>)\n",
      "epoch:  1740 loss:\t tensor(0.5905, grad_fn=<NllLossBackward>)\n",
      "epoch:  1750 loss:\t tensor(0.6352, grad_fn=<NllLossBackward>)\n",
      "epoch:  1760 loss:\t tensor(0.9143, grad_fn=<NllLossBackward>)\n",
      "epoch:  1770 loss:\t tensor(0.5371, grad_fn=<NllLossBackward>)\n",
      "epoch:  1780 loss:\t tensor(1.2566, grad_fn=<NllLossBackward>)\n",
      "epoch:  1790 loss:\t tensor(0.8503, grad_fn=<NllLossBackward>)\n",
      "epoch:  1800 loss:\t tensor(0.7263, grad_fn=<NllLossBackward>)\n",
      "epoch:  1810 loss:\t tensor(0.6419, grad_fn=<NllLossBackward>)\n",
      "epoch:  1820 loss:\t tensor(0.8113, grad_fn=<NllLossBackward>)\n",
      "epoch:  1830 loss:\t tensor(0.5912, grad_fn=<NllLossBackward>)\n",
      "epoch:  1840 loss:\t tensor(0.4685, grad_fn=<NllLossBackward>)\n",
      "epoch:  1850 loss:\t tensor(1.0538, grad_fn=<NllLossBackward>)\n",
      "epoch:  1860 loss:\t tensor(0.5571, grad_fn=<NllLossBackward>)\n",
      "epoch:  1870 loss:\t tensor(0.6542, grad_fn=<NllLossBackward>)\n",
      "epoch:  1880 loss:\t tensor(1.6624, grad_fn=<NllLossBackward>)\n",
      "epoch:  1890 loss:\t tensor(0.7205, grad_fn=<NllLossBackward>)\n",
      "epoch:  1900 loss:\t tensor(0.8395, grad_fn=<NllLossBackward>)\n",
      "epoch:  1910 loss:\t tensor(0.8032, grad_fn=<NllLossBackward>)\n",
      "epoch:  1920 loss:\t tensor(0.8210, grad_fn=<NllLossBackward>)\n",
      "epoch:  1930 loss:\t tensor(0.7627, grad_fn=<NllLossBackward>)\n",
      "epoch:  1940 loss:\t tensor(0.6430, grad_fn=<NllLossBackward>)\n",
      "epoch:  1950 loss:\t tensor(0.7825, grad_fn=<NllLossBackward>)\n",
      "epoch:  1960 loss:\t tensor(0.7289, grad_fn=<NllLossBackward>)\n",
      "epoch:  1970 loss:\t tensor(0.4699, grad_fn=<NllLossBackward>)\n",
      "epoch:  1980 loss:\t tensor(0.4403, grad_fn=<NllLossBackward>)\n",
      "epoch:  1990 loss:\t tensor(1.0426, grad_fn=<NllLossBackward>)\n",
      "epoch:  2000 loss:\t tensor(1.0054, grad_fn=<NllLossBackward>)\n",
      "epoch:  2010 loss:\t tensor(0.6736, grad_fn=<NllLossBackward>)\n",
      "epoch:  2020 loss:\t tensor(0.6492, grad_fn=<NllLossBackward>)\n",
      "epoch:  2030 loss:\t tensor(0.7633, grad_fn=<NllLossBackward>)\n",
      "epoch:  2040 loss:\t tensor(0.8538, grad_fn=<NllLossBackward>)\n",
      "epoch:  2050 loss:\t tensor(0.7289, grad_fn=<NllLossBackward>)\n",
      "epoch:  2060 loss:\t tensor(0.6859, grad_fn=<NllLossBackward>)\n",
      "epoch:  2070 loss:\t tensor(0.7559, grad_fn=<NllLossBackward>)\n",
      "epoch:  2080 loss:\t tensor(0.9858, grad_fn=<NllLossBackward>)\n",
      "epoch:  2090 loss:\t tensor(0.5617, grad_fn=<NllLossBackward>)\n",
      "epoch:  2100 loss:\t tensor(0.6373, grad_fn=<NllLossBackward>)\n",
      "epoch:  2110 loss:\t tensor(0.9076, grad_fn=<NllLossBackward>)\n",
      "epoch:  2120 loss:\t tensor(0.7211, grad_fn=<NllLossBackward>)\n",
      "epoch:  2130 loss:\t tensor(0.8561, grad_fn=<NllLossBackward>)\n",
      "epoch:  2140 loss:\t tensor(0.6238, grad_fn=<NllLossBackward>)\n",
      "epoch:  2150 loss:\t tensor(0.9019, grad_fn=<NllLossBackward>)\n",
      "epoch:  2160 loss:\t tensor(0.4771, grad_fn=<NllLossBackward>)\n",
      "epoch:  2170 loss:\t tensor(0.6667, grad_fn=<NllLossBackward>)\n",
      "epoch:  2180 loss:\t tensor(0.7932, grad_fn=<NllLossBackward>)\n",
      "epoch:  2190 loss:\t tensor(0.7851, grad_fn=<NllLossBackward>)\n",
      "epoch:  2200 loss:\t tensor(0.4817, grad_fn=<NllLossBackward>)\n",
      "epoch:  2210 loss:\t tensor(0.3832, grad_fn=<NllLossBackward>)\n",
      "epoch:  2220 loss:\t tensor(1.0872, grad_fn=<NllLossBackward>)\n",
      "epoch:  2230 loss:\t tensor(1.0086, grad_fn=<NllLossBackward>)\n",
      "epoch:  2240 loss:\t tensor(0.6881, grad_fn=<NllLossBackward>)\n",
      "epoch:  2250 loss:\t tensor(0.8692, grad_fn=<NllLossBackward>)\n",
      "epoch:  2260 loss:\t tensor(0.9468, grad_fn=<NllLossBackward>)\n",
      "epoch:  2270 loss:\t tensor(0.7290, grad_fn=<NllLossBackward>)\n",
      "epoch:  2280 loss:\t tensor(0.9049, grad_fn=<NllLossBackward>)\n",
      "epoch:  2290 loss:\t tensor(0.8453, grad_fn=<NllLossBackward>)\n",
      "epoch:  2300 loss:\t tensor(0.8613, grad_fn=<NllLossBackward>)\n",
      "epoch:  2310 loss:\t tensor(0.6726, grad_fn=<NllLossBackward>)\n",
      "epoch:  2320 loss:\t tensor(0.9542, grad_fn=<NllLossBackward>)\n",
      "epoch:  2330 loss:\t tensor(1.0416, grad_fn=<NllLossBackward>)\n",
      "epoch:  2340 loss:\t tensor(0.8331, grad_fn=<NllLossBackward>)\n",
      "epoch:  2350 loss:\t tensor(0.5589, grad_fn=<NllLossBackward>)\n",
      "epoch:  2360 loss:\t tensor(1.1124, grad_fn=<NllLossBackward>)\n",
      "epoch:  2370 loss:\t tensor(1.2689, grad_fn=<NllLossBackward>)\n",
      "epoch:  2380 loss:\t tensor(0.5852, grad_fn=<NllLossBackward>)\n",
      "epoch:  2390 loss:\t tensor(0.9405, grad_fn=<NllLossBackward>)\n",
      "epoch:  2400 loss:\t tensor(0.8651, grad_fn=<NllLossBackward>)\n",
      "epoch:  2410 loss:\t tensor(0.8265, grad_fn=<NllLossBackward>)\n",
      "epoch:  2420 loss:\t tensor(1.0596, grad_fn=<NllLossBackward>)\n",
      "epoch:  2430 loss:\t tensor(0.5316, grad_fn=<NllLossBackward>)\n",
      "epoch:  2440 loss:\t tensor(0.8432, grad_fn=<NllLossBackward>)\n",
      "epoch:  2450 loss:\t tensor(0.6617, grad_fn=<NllLossBackward>)\n",
      "epoch:  2460 loss:\t tensor(0.9961, grad_fn=<NllLossBackward>)\n",
      "epoch:  2470 loss:\t tensor(0.5814, grad_fn=<NllLossBackward>)\n",
      "epoch:  2480 loss:\t tensor(0.8622, grad_fn=<NllLossBackward>)\n",
      "epoch:  2490 loss:\t tensor(0.7013, grad_fn=<NllLossBackward>)\n",
      "epoch:  2500 loss:\t tensor(1.1008, grad_fn=<NllLossBackward>)\n",
      "epoch:  2510 loss:\t tensor(1.0598, grad_fn=<NllLossBackward>)\n",
      "epoch:  2520 loss:\t tensor(0.5639, grad_fn=<NllLossBackward>)\n",
      "epoch:  2530 loss:\t tensor(0.8427, grad_fn=<NllLossBackward>)\n",
      "epoch:  2540 loss:\t tensor(0.5638, grad_fn=<NllLossBackward>)\n",
      "epoch:  2550 loss:\t tensor(0.9406, grad_fn=<NllLossBackward>)\n",
      "epoch:  2560 loss:\t tensor(0.6987, grad_fn=<NllLossBackward>)\n",
      "epoch:  2570 loss:\t tensor(0.6479, grad_fn=<NllLossBackward>)\n",
      "epoch:  2580 loss:\t tensor(0.9797, grad_fn=<NllLossBackward>)\n",
      "epoch:  2590 loss:\t tensor(0.5875, grad_fn=<NllLossBackward>)\n",
      "epoch:  2600 loss:\t tensor(1.2397, grad_fn=<NllLossBackward>)\n",
      "epoch:  2610 loss:\t tensor(0.4273, grad_fn=<NllLossBackward>)\n",
      "epoch:  2620 loss:\t tensor(0.5450, grad_fn=<NllLossBackward>)\n",
      "epoch:  2630 loss:\t tensor(0.3930, grad_fn=<NllLossBackward>)\n",
      "epoch:  2640 loss:\t tensor(0.7315, grad_fn=<NllLossBackward>)\n",
      "epoch:  2650 loss:\t tensor(0.9866, grad_fn=<NllLossBackward>)\n",
      "epoch:  2660 loss:\t tensor(0.5216, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  2670 loss:\t tensor(0.6025, grad_fn=<NllLossBackward>)\n",
      "epoch:  2680 loss:\t tensor(0.9994, grad_fn=<NllLossBackward>)\n",
      "epoch:  2690 loss:\t tensor(0.4283, grad_fn=<NllLossBackward>)\n",
      "epoch:  2700 loss:\t tensor(0.5325, grad_fn=<NllLossBackward>)\n",
      "epoch:  2710 loss:\t tensor(0.8636, grad_fn=<NllLossBackward>)\n",
      "epoch:  2720 loss:\t tensor(0.5102, grad_fn=<NllLossBackward>)\n",
      "epoch:  2730 loss:\t tensor(1.0014, grad_fn=<NllLossBackward>)\n",
      "epoch:  2740 loss:\t tensor(0.7030, grad_fn=<NllLossBackward>)\n",
      "epoch:  2750 loss:\t tensor(1.0579, grad_fn=<NllLossBackward>)\n",
      "epoch:  2760 loss:\t tensor(0.9251, grad_fn=<NllLossBackward>)\n",
      "epoch:  2770 loss:\t tensor(0.5229, grad_fn=<NllLossBackward>)\n",
      "epoch:  2780 loss:\t tensor(0.6246, grad_fn=<NllLossBackward>)\n",
      "epoch:  2790 loss:\t tensor(0.6717, grad_fn=<NllLossBackward>)\n",
      "epoch:  2800 loss:\t tensor(0.6780, grad_fn=<NllLossBackward>)\n",
      "epoch:  2810 loss:\t tensor(0.8467, grad_fn=<NllLossBackward>)\n",
      "epoch:  2820 loss:\t tensor(0.4601, grad_fn=<NllLossBackward>)\n",
      "epoch:  2830 loss:\t tensor(0.3993, grad_fn=<NllLossBackward>)\n",
      "epoch:  2840 loss:\t tensor(0.4256, grad_fn=<NllLossBackward>)\n",
      "epoch:  2850 loss:\t tensor(0.6815, grad_fn=<NllLossBackward>)\n",
      "epoch:  2860 loss:\t tensor(0.5729, grad_fn=<NllLossBackward>)\n",
      "epoch:  2870 loss:\t tensor(0.9961, grad_fn=<NllLossBackward>)\n",
      "epoch:  2880 loss:\t tensor(0.6730, grad_fn=<NllLossBackward>)\n",
      "epoch:  2890 loss:\t tensor(0.6276, grad_fn=<NllLossBackward>)\n",
      "epoch:  2900 loss:\t tensor(0.5499, grad_fn=<NllLossBackward>)\n",
      "epoch:  2910 loss:\t tensor(1.2798, grad_fn=<NllLossBackward>)\n",
      "epoch:  2920 loss:\t tensor(0.9818, grad_fn=<NllLossBackward>)\n",
      "epoch:  2930 loss:\t tensor(0.6155, grad_fn=<NllLossBackward>)\n",
      "epoch:  2940 loss:\t tensor(0.6553, grad_fn=<NllLossBackward>)\n",
      "epoch:  2950 loss:\t tensor(0.7411, grad_fn=<NllLossBackward>)\n",
      "epoch:  2960 loss:\t tensor(0.4941, grad_fn=<NllLossBackward>)\n",
      "epoch:  2970 loss:\t tensor(0.4057, grad_fn=<NllLossBackward>)\n",
      "epoch:  2980 loss:\t tensor(1.1618, grad_fn=<NllLossBackward>)\n",
      "epoch:  2990 loss:\t tensor(1.7935, grad_fn=<NllLossBackward>)\n",
      "epoch:  3000 loss:\t tensor(0.6832, grad_fn=<NllLossBackward>)\n",
      "epoch:  3010 loss:\t tensor(0.8769, grad_fn=<NllLossBackward>)\n",
      "epoch:  3020 loss:\t tensor(1.0453, grad_fn=<NllLossBackward>)\n",
      "epoch:  3030 loss:\t tensor(0.8524, grad_fn=<NllLossBackward>)\n",
      "epoch:  3040 loss:\t tensor(0.6762, grad_fn=<NllLossBackward>)\n",
      "epoch:  3050 loss:\t tensor(0.6280, grad_fn=<NllLossBackward>)\n",
      "epoch:  3060 loss:\t tensor(0.6901, grad_fn=<NllLossBackward>)\n",
      "epoch:  3070 loss:\t tensor(0.8424, grad_fn=<NllLossBackward>)\n",
      "epoch:  3080 loss:\t tensor(0.4210, grad_fn=<NllLossBackward>)\n",
      "epoch:  3090 loss:\t tensor(0.7312, grad_fn=<NllLossBackward>)\n",
      "epoch:  3100 loss:\t tensor(0.8130, grad_fn=<NllLossBackward>)\n",
      "epoch:  3110 loss:\t tensor(0.4836, grad_fn=<NllLossBackward>)\n",
      "epoch:  3120 loss:\t tensor(1.0796, grad_fn=<NllLossBackward>)\n",
      "epoch:  3130 loss:\t tensor(0.9527, grad_fn=<NllLossBackward>)\n",
      "epoch:  3140 loss:\t tensor(0.9050, grad_fn=<NllLossBackward>)\n",
      "epoch:  3150 loss:\t tensor(0.8076, grad_fn=<NllLossBackward>)\n",
      "epoch:  3160 loss:\t tensor(0.3158, grad_fn=<NllLossBackward>)\n",
      "epoch:  3170 loss:\t tensor(1.1946, grad_fn=<NllLossBackward>)\n",
      "epoch:  3180 loss:\t tensor(0.9313, grad_fn=<NllLossBackward>)\n",
      "epoch:  3190 loss:\t tensor(0.6941, grad_fn=<NllLossBackward>)\n",
      "epoch:  3200 loss:\t tensor(0.6854, grad_fn=<NllLossBackward>)\n",
      "epoch:  3210 loss:\t tensor(0.5419, grad_fn=<NllLossBackward>)\n",
      "epoch:  3220 loss:\t tensor(0.4531, grad_fn=<NllLossBackward>)\n",
      "epoch:  3230 loss:\t tensor(0.7250, grad_fn=<NllLossBackward>)\n",
      "epoch:  3240 loss:\t tensor(0.6911, grad_fn=<NllLossBackward>)\n",
      "epoch:  3250 loss:\t tensor(0.8016, grad_fn=<NllLossBackward>)\n",
      "epoch:  3260 loss:\t tensor(1.1178, grad_fn=<NllLossBackward>)\n",
      "epoch:  3270 loss:\t tensor(0.6723, grad_fn=<NllLossBackward>)\n",
      "epoch:  3280 loss:\t tensor(0.5709, grad_fn=<NllLossBackward>)\n",
      "epoch:  3290 loss:\t tensor(0.4915, grad_fn=<NllLossBackward>)\n",
      "epoch:  3300 loss:\t tensor(0.4092, grad_fn=<NllLossBackward>)\n",
      "epoch:  3310 loss:\t tensor(0.8619, grad_fn=<NllLossBackward>)\n",
      "epoch:  3320 loss:\t tensor(0.4757, grad_fn=<NllLossBackward>)\n",
      "epoch:  3330 loss:\t tensor(0.4685, grad_fn=<NllLossBackward>)\n",
      "epoch:  3340 loss:\t tensor(0.4759, grad_fn=<NllLossBackward>)\n",
      "epoch:  3350 loss:\t tensor(0.5684, grad_fn=<NllLossBackward>)\n",
      "epoch:  3360 loss:\t tensor(0.6518, grad_fn=<NllLossBackward>)\n",
      "epoch:  3370 loss:\t tensor(0.6564, grad_fn=<NllLossBackward>)\n",
      "epoch:  3380 loss:\t tensor(0.7137, grad_fn=<NllLossBackward>)\n",
      "epoch:  3390 loss:\t tensor(0.6690, grad_fn=<NllLossBackward>)\n",
      "epoch:  3400 loss:\t tensor(0.7526, grad_fn=<NllLossBackward>)\n",
      "epoch:  3410 loss:\t tensor(0.8973, grad_fn=<NllLossBackward>)\n",
      "epoch:  3420 loss:\t tensor(0.5947, grad_fn=<NllLossBackward>)\n",
      "epoch:  3430 loss:\t tensor(1.1271, grad_fn=<NllLossBackward>)\n",
      "epoch:  3440 loss:\t tensor(0.4862, grad_fn=<NllLossBackward>)\n",
      "epoch:  3450 loss:\t tensor(0.9746, grad_fn=<NllLossBackward>)\n",
      "epoch:  3460 loss:\t tensor(0.5718, grad_fn=<NllLossBackward>)\n",
      "epoch:  3470 loss:\t tensor(0.7492, grad_fn=<NllLossBackward>)\n",
      "epoch:  3480 loss:\t tensor(0.5251, grad_fn=<NllLossBackward>)\n",
      "epoch:  3490 loss:\t tensor(0.9169, grad_fn=<NllLossBackward>)\n",
      "epoch:  3500 loss:\t tensor(0.6340, grad_fn=<NllLossBackward>)\n",
      "epoch:  3510 loss:\t tensor(0.7967, grad_fn=<NllLossBackward>)\n",
      "epoch:  3520 loss:\t tensor(0.7162, grad_fn=<NllLossBackward>)\n",
      "epoch:  3530 loss:\t tensor(0.7126, grad_fn=<NllLossBackward>)\n",
      "epoch:  3540 loss:\t tensor(1.1681, grad_fn=<NllLossBackward>)\n",
      "epoch:  3550 loss:\t tensor(0.6200, grad_fn=<NllLossBackward>)\n",
      "epoch:  3560 loss:\t tensor(0.9974, grad_fn=<NllLossBackward>)\n",
      "epoch:  3570 loss:\t tensor(0.6016, grad_fn=<NllLossBackward>)\n",
      "epoch:  3580 loss:\t tensor(0.6363, grad_fn=<NllLossBackward>)\n",
      "epoch:  3590 loss:\t tensor(0.5262, grad_fn=<NllLossBackward>)\n",
      "epoch:  3600 loss:\t tensor(0.6097, grad_fn=<NllLossBackward>)\n",
      "epoch:  3610 loss:\t tensor(0.3617, grad_fn=<NllLossBackward>)\n",
      "epoch:  3620 loss:\t tensor(1.0951, grad_fn=<NllLossBackward>)\n",
      "epoch:  3630 loss:\t tensor(0.5253, grad_fn=<NllLossBackward>)\n",
      "epoch:  3640 loss:\t tensor(0.6697, grad_fn=<NllLossBackward>)\n",
      "epoch:  3650 loss:\t tensor(0.3830, grad_fn=<NllLossBackward>)\n",
      "epoch:  3660 loss:\t tensor(1.1428, grad_fn=<NllLossBackward>)\n",
      "epoch:  3670 loss:\t tensor(0.5063, grad_fn=<NllLossBackward>)\n",
      "epoch:  3680 loss:\t tensor(0.6042, grad_fn=<NllLossBackward>)\n",
      "epoch:  3690 loss:\t tensor(0.6483, grad_fn=<NllLossBackward>)\n",
      "epoch:  3700 loss:\t tensor(0.9542, grad_fn=<NllLossBackward>)\n",
      "epoch:  3710 loss:\t tensor(0.4542, grad_fn=<NllLossBackward>)\n",
      "epoch:  3720 loss:\t tensor(0.6811, grad_fn=<NllLossBackward>)\n",
      "epoch:  3730 loss:\t tensor(0.7695, grad_fn=<NllLossBackward>)\n",
      "epoch:  3740 loss:\t tensor(0.4878, grad_fn=<NllLossBackward>)\n",
      "epoch:  3750 loss:\t tensor(0.6709, grad_fn=<NllLossBackward>)\n",
      "epoch:  3760 loss:\t tensor(0.9992, grad_fn=<NllLossBackward>)\n",
      "epoch:  3770 loss:\t tensor(0.8032, grad_fn=<NllLossBackward>)\n",
      "epoch:  3780 loss:\t tensor(0.9421, grad_fn=<NllLossBackward>)\n",
      "epoch:  3790 loss:\t tensor(0.7489, grad_fn=<NllLossBackward>)\n",
      "epoch:  3800 loss:\t tensor(0.4098, grad_fn=<NllLossBackward>)\n",
      "epoch:  3810 loss:\t tensor(0.6348, grad_fn=<NllLossBackward>)\n",
      "epoch:  3820 loss:\t tensor(0.9470, grad_fn=<NllLossBackward>)\n",
      "epoch:  3830 loss:\t tensor(0.6463, grad_fn=<NllLossBackward>)\n",
      "epoch:  3840 loss:\t tensor(0.9322, grad_fn=<NllLossBackward>)\n",
      "epoch:  3850 loss:\t tensor(0.3890, grad_fn=<NllLossBackward>)\n",
      "epoch:  3860 loss:\t tensor(0.5889, grad_fn=<NllLossBackward>)\n",
      "epoch:  3870 loss:\t tensor(0.8410, grad_fn=<NllLossBackward>)\n",
      "epoch:  3880 loss:\t tensor(0.4585, grad_fn=<NllLossBackward>)\n",
      "epoch:  3890 loss:\t tensor(0.7836, grad_fn=<NllLossBackward>)\n",
      "epoch:  3900 loss:\t tensor(1.1215, grad_fn=<NllLossBackward>)\n",
      "epoch:  3910 loss:\t tensor(0.7891, grad_fn=<NllLossBackward>)\n",
      "epoch:  3920 loss:\t tensor(0.6253, grad_fn=<NllLossBackward>)\n",
      "epoch:  3930 loss:\t tensor(1.3059, grad_fn=<NllLossBackward>)\n",
      "epoch:  3940 loss:\t tensor(0.6131, grad_fn=<NllLossBackward>)\n",
      "epoch:  3950 loss:\t tensor(0.8390, grad_fn=<NllLossBackward>)\n",
      "epoch:  3960 loss:\t tensor(0.7959, grad_fn=<NllLossBackward>)\n",
      "epoch:  3970 loss:\t tensor(0.5611, grad_fn=<NllLossBackward>)\n",
      "epoch:  3980 loss:\t tensor(0.6592, grad_fn=<NllLossBackward>)\n",
      "epoch:  3990 loss:\t tensor(0.8750, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  4000 loss:\t tensor(0.6601, grad_fn=<NllLossBackward>)\n",
      "epoch:  4010 loss:\t tensor(0.5317, grad_fn=<NllLossBackward>)\n",
      "epoch:  4020 loss:\t tensor(0.5878, grad_fn=<NllLossBackward>)\n",
      "epoch:  4030 loss:\t tensor(1.2726, grad_fn=<NllLossBackward>)\n",
      "epoch:  4040 loss:\t tensor(0.6139, grad_fn=<NllLossBackward>)\n",
      "epoch:  4050 loss:\t tensor(0.3647, grad_fn=<NllLossBackward>)\n",
      "epoch:  4060 loss:\t tensor(0.8824, grad_fn=<NllLossBackward>)\n",
      "epoch:  4070 loss:\t tensor(0.7432, grad_fn=<NllLossBackward>)\n",
      "epoch:  4080 loss:\t tensor(0.6143, grad_fn=<NllLossBackward>)\n",
      "epoch:  4090 loss:\t tensor(0.4771, grad_fn=<NllLossBackward>)\n",
      "epoch:  4100 loss:\t tensor(0.6824, grad_fn=<NllLossBackward>)\n",
      "epoch:  4110 loss:\t tensor(0.6246, grad_fn=<NllLossBackward>)\n",
      "epoch:  4120 loss:\t tensor(0.4678, grad_fn=<NllLossBackward>)\n",
      "epoch:  4130 loss:\t tensor(0.4986, grad_fn=<NllLossBackward>)\n",
      "epoch:  4140 loss:\t tensor(0.6650, grad_fn=<NllLossBackward>)\n",
      "epoch:  4150 loss:\t tensor(0.5521, grad_fn=<NllLossBackward>)\n",
      "epoch:  4160 loss:\t tensor(0.3903, grad_fn=<NllLossBackward>)\n",
      "epoch:  4170 loss:\t tensor(0.8785, grad_fn=<NllLossBackward>)\n",
      "epoch:  4180 loss:\t tensor(0.5587, grad_fn=<NllLossBackward>)\n",
      "epoch:  4190 loss:\t tensor(1.1783, grad_fn=<NllLossBackward>)\n",
      "epoch:  4200 loss:\t tensor(0.6879, grad_fn=<NllLossBackward>)\n",
      "epoch:  4210 loss:\t tensor(0.5278, grad_fn=<NllLossBackward>)\n",
      "epoch:  4220 loss:\t tensor(0.5678, grad_fn=<NllLossBackward>)\n",
      "epoch:  4230 loss:\t tensor(1.1910, grad_fn=<NllLossBackward>)\n",
      "epoch:  4240 loss:\t tensor(0.8468, grad_fn=<NllLossBackward>)\n",
      "epoch:  4250 loss:\t tensor(0.7717, grad_fn=<NllLossBackward>)\n",
      "epoch:  4260 loss:\t tensor(0.6681, grad_fn=<NllLossBackward>)\n",
      "epoch:  4270 loss:\t tensor(0.9030, grad_fn=<NllLossBackward>)\n",
      "epoch:  4280 loss:\t tensor(0.7872, grad_fn=<NllLossBackward>)\n",
      "epoch:  4290 loss:\t tensor(0.8051, grad_fn=<NllLossBackward>)\n",
      "epoch:  4300 loss:\t tensor(0.8253, grad_fn=<NllLossBackward>)\n",
      "epoch:  4310 loss:\t tensor(0.9225, grad_fn=<NllLossBackward>)\n",
      "epoch:  4320 loss:\t tensor(0.6947, grad_fn=<NllLossBackward>)\n",
      "epoch:  4330 loss:\t tensor(0.8807, grad_fn=<NllLossBackward>)\n",
      "epoch:  4340 loss:\t tensor(0.9741, grad_fn=<NllLossBackward>)\n",
      "epoch:  4350 loss:\t tensor(0.9914, grad_fn=<NllLossBackward>)\n",
      "epoch:  4360 loss:\t tensor(1.1563, grad_fn=<NllLossBackward>)\n",
      "epoch:  4370 loss:\t tensor(0.4157, grad_fn=<NllLossBackward>)\n",
      "epoch:  4380 loss:\t tensor(0.8831, grad_fn=<NllLossBackward>)\n",
      "epoch:  4390 loss:\t tensor(1.0190, grad_fn=<NllLossBackward>)\n",
      "epoch:  4400 loss:\t tensor(0.5401, grad_fn=<NllLossBackward>)\n",
      "epoch:  4410 loss:\t tensor(0.8721, grad_fn=<NllLossBackward>)\n",
      "epoch:  4420 loss:\t tensor(0.4985, grad_fn=<NllLossBackward>)\n",
      "epoch:  4430 loss:\t tensor(0.9865, grad_fn=<NllLossBackward>)\n",
      "epoch:  4440 loss:\t tensor(0.3545, grad_fn=<NllLossBackward>)\n",
      "epoch:  4450 loss:\t tensor(1.4822, grad_fn=<NllLossBackward>)\n",
      "epoch:  4460 loss:\t tensor(0.5569, grad_fn=<NllLossBackward>)\n",
      "epoch:  4470 loss:\t tensor(1.0028, grad_fn=<NllLossBackward>)\n",
      "epoch:  4480 loss:\t tensor(0.5562, grad_fn=<NllLossBackward>)\n",
      "epoch:  4490 loss:\t tensor(0.8699, grad_fn=<NllLossBackward>)\n",
      "epoch:  4500 loss:\t tensor(0.7309, grad_fn=<NllLossBackward>)\n",
      "epoch:  4510 loss:\t tensor(0.6532, grad_fn=<NllLossBackward>)\n",
      "epoch:  4520 loss:\t tensor(0.8095, grad_fn=<NllLossBackward>)\n",
      "epoch:  4530 loss:\t tensor(0.4371, grad_fn=<NllLossBackward>)\n",
      "epoch:  4540 loss:\t tensor(0.3984, grad_fn=<NllLossBackward>)\n",
      "epoch:  4550 loss:\t tensor(0.7973, grad_fn=<NllLossBackward>)\n",
      "epoch:  4560 loss:\t tensor(0.9223, grad_fn=<NllLossBackward>)\n",
      "epoch:  4570 loss:\t tensor(0.3705, grad_fn=<NllLossBackward>)\n",
      "epoch:  4580 loss:\t tensor(0.6832, grad_fn=<NllLossBackward>)\n",
      "epoch:  4590 loss:\t tensor(0.7502, grad_fn=<NllLossBackward>)\n",
      "epoch:  4600 loss:\t tensor(0.5132, grad_fn=<NllLossBackward>)\n",
      "epoch:  4610 loss:\t tensor(1.2668, grad_fn=<NllLossBackward>)\n",
      "epoch:  4620 loss:\t tensor(1.1195, grad_fn=<NllLossBackward>)\n",
      "epoch:  4630 loss:\t tensor(0.6495, grad_fn=<NllLossBackward>)\n",
      "epoch:  4640 loss:\t tensor(0.6549, grad_fn=<NllLossBackward>)\n",
      "epoch:  4650 loss:\t tensor(0.6549, grad_fn=<NllLossBackward>)\n",
      "epoch:  4660 loss:\t tensor(0.6376, grad_fn=<NllLossBackward>)\n",
      "epoch:  4670 loss:\t tensor(0.9742, grad_fn=<NllLossBackward>)\n",
      "epoch:  4680 loss:\t tensor(0.6046, grad_fn=<NllLossBackward>)\n",
      "epoch:  4690 loss:\t tensor(0.7648, grad_fn=<NllLossBackward>)\n",
      "epoch:  4700 loss:\t tensor(0.5365, grad_fn=<NllLossBackward>)\n",
      "epoch:  4710 loss:\t tensor(0.9682, grad_fn=<NllLossBackward>)\n",
      "epoch:  4720 loss:\t tensor(0.5199, grad_fn=<NllLossBackward>)\n",
      "epoch:  4730 loss:\t tensor(0.6834, grad_fn=<NllLossBackward>)\n",
      "epoch:  4740 loss:\t tensor(0.8739, grad_fn=<NllLossBackward>)\n",
      "epoch:  4750 loss:\t tensor(0.5979, grad_fn=<NllLossBackward>)\n",
      "epoch:  4760 loss:\t tensor(0.4861, grad_fn=<NllLossBackward>)\n",
      "epoch:  4770 loss:\t tensor(0.4901, grad_fn=<NllLossBackward>)\n",
      "epoch:  4780 loss:\t tensor(0.6684, grad_fn=<NllLossBackward>)\n",
      "epoch:  4790 loss:\t tensor(0.8638, grad_fn=<NllLossBackward>)\n",
      "epoch:  4800 loss:\t tensor(0.4085, grad_fn=<NllLossBackward>)\n",
      "epoch:  4810 loss:\t tensor(0.5898, grad_fn=<NllLossBackward>)\n",
      "epoch:  4820 loss:\t tensor(0.7566, grad_fn=<NllLossBackward>)\n",
      "epoch:  4830 loss:\t tensor(0.6278, grad_fn=<NllLossBackward>)\n",
      "epoch:  4840 loss:\t tensor(0.5152, grad_fn=<NllLossBackward>)\n",
      "epoch:  4850 loss:\t tensor(0.4878, grad_fn=<NllLossBackward>)\n",
      "epoch:  4860 loss:\t tensor(0.6321, grad_fn=<NllLossBackward>)\n",
      "epoch:  4870 loss:\t tensor(0.5711, grad_fn=<NllLossBackward>)\n",
      "epoch:  4880 loss:\t tensor(0.5449, grad_fn=<NllLossBackward>)\n",
      "epoch:  4890 loss:\t tensor(1.1467, grad_fn=<NllLossBackward>)\n",
      "epoch:  4900 loss:\t tensor(0.4707, grad_fn=<NllLossBackward>)\n",
      "epoch:  4910 loss:\t tensor(0.4499, grad_fn=<NllLossBackward>)\n",
      "epoch:  4920 loss:\t tensor(0.4928, grad_fn=<NllLossBackward>)\n",
      "epoch:  4930 loss:\t tensor(0.4343, grad_fn=<NllLossBackward>)\n",
      "epoch:  4940 loss:\t tensor(1.0625, grad_fn=<NllLossBackward>)\n",
      "epoch:  4950 loss:\t tensor(0.8691, grad_fn=<NllLossBackward>)\n",
      "epoch:  4960 loss:\t tensor(0.5595, grad_fn=<NllLossBackward>)\n",
      "epoch:  4970 loss:\t tensor(0.7825, grad_fn=<NllLossBackward>)\n",
      "epoch:  4980 loss:\t tensor(0.3396, grad_fn=<NllLossBackward>)\n",
      "epoch:  4990 loss:\t tensor(0.6447, grad_fn=<NllLossBackward>)\n",
      "CPU times: user 5h 22min 17s, sys: 2h 4min 58s, total: 7h 27min 16s\n",
      "Wall time: 2h 33min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.train()\n",
    "\n",
    "for e in range(1000):\n",
    "    for d in train_loader:        \n",
    "        optim.zero_grad()\n",
    "        features, labels = d\n",
    "        pred = model(features)\n",
    "        loss = loss_f(pred, labels)\n",
    "        errors.append(loss.item())\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        batches += 1\n",
    "    epochs += 1   \n",
    "    if e%10 ==0:\n",
    "        print(\"epoch: \", e,\"loss:\\t\", loss)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f89a25c6208>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.linspace(0,epochs, batches),errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7270)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = torch.softmax(model(train_dataset[:][0]),1)\n",
    "    pred_labels= torch.argmax(pred,1)\n",
    "    true_label = train_dataset[:][1]\n",
    "    \n",
    "    ac = torch.sum(torch.argmax(pred,1)==train_dataset[:][1]).to(dtype=torch.float32)/len(train_dataset)\n",
    "ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6750)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = torch.softmax(model(validation_dataset[:][0]),1)\n",
    "    ac = torch.sum(torch.argmax(pred,1)==validation_dataset[:][1]).to(dtype=torch.float32)/len(validation_dataset)\n",
    "ac    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
