{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "n6enO0B4xsf5",
    "outputId": "8ee9834e-ef94-47b8-a632-25e2226d3108"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.5'"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "h8vS9wEzxte1",
    "outputId": "0d24886f-5980-4a14-d625-baccb08a489c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_07G2xDMyC2i"
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras.models as models\n",
    "import tensorflow.keras.layers as layers \n",
    "import tensorflow.keras.optimizers as optimizers\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "eyifxj2ByKaw",
    "outputId": "585ad425-0702-4833-dd41-a400ff6a9ec4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: images (40000, 32, 32, 3)\tlabels: (40000, 10)\n",
      "validation set: images (10000, 32, 32, 3)\tlabels: (10000, 10)\n",
      "Test set: images (10000, 32, 32, 3)\tlabels: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "\n",
    "validation_images = train_images[40000:]\n",
    "validation_labels = train_labels[40000:]\n",
    "\n",
    "train_images = train_images[:40000]\n",
    "train_labels = train_labels[:40000]\n",
    "\n",
    "train_images = train_images.reshape((40000, 32, 32, 3))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "validation_images = validation_images.reshape((10000, 32, 32, 3))\n",
    "validation_images = validation_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 32, 32, 3))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "validation_labels = to_categorical(validation_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "print(\"Training set: images {:}\\tlabels: {:}\".format(train_images.shape, train_labels.shape ) )\n",
    "print(\"validation set: images {:}\\tlabels: {:}\".format(validation_images.shape, validation_labels.shape ) )\n",
    "print(\"Test set: images {:}\\tlabels: {:}\".format(test_images.shape, test_labels.shape ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 920
    },
    "colab_type": "code",
    "id": "bgEKMx2qxth7",
    "outputId": "cafee291-8575-4cdc-bb80-a77d6d133cc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 20,024,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG19\n",
    "\n",
    "conv_base = VGG19(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(32, 32, 3))\n",
    "\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "colab_type": "code",
    "id": "RxYgqrIv0pve",
    "outputId": "2ff99a5a-8e26-4f0f-e957-77c116bf8725"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg19 (Model)                (None, 1, 1, 512)         20024384  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 20,452,554\n",
      "Trainable params: 428,170\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "model.add(conv_base)\n",
    "conv_base.trainable = False\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=5e-4), \n",
    "              loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "L3EwubwVWETB",
    "outputId": "a611c50a-e103-40d1-e021-4fa6746b6106"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/60\n",
      "40000/40000 [==============================] - 5s 114us/sample - loss: 2.1595 - acc: 0.2072 - val_loss: 1.8082 - val_acc: 0.3913\n",
      "Epoch 2/60\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 1.8577 - acc: 0.3265 - val_loss: 1.6148 - val_acc: 0.4279\n",
      "Epoch 3/60\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 1.7166 - acc: 0.3785 - val_loss: 1.5037 - val_acc: 0.4680\n",
      "Epoch 4/60\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 1.6320 - acc: 0.4193 - val_loss: 1.4445 - val_acc: 0.4882\n",
      "Epoch 5/60\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 1.5793 - acc: 0.4398 - val_loss: 1.4267 - val_acc: 0.4927\n",
      "Epoch 6/60\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 1.5295 - acc: 0.4621 - val_loss: 1.3852 - val_acc: 0.5042\n",
      "Epoch 7/60\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 1.4975 - acc: 0.4771 - val_loss: 1.3448 - val_acc: 0.5207\n",
      "Epoch 8/60\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 1.4670 - acc: 0.4927 - val_loss: 1.3540 - val_acc: 0.5233\n",
      "Epoch 9/60\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 1.4383 - acc: 0.5038 - val_loss: 1.3428 - val_acc: 0.5249\n",
      "Epoch 10/60\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 1.4106 - acc: 0.5133 - val_loss: 1.3256 - val_acc: 0.5311\n",
      "Epoch 11/60\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 1.3981 - acc: 0.5186 - val_loss: 1.2986 - val_acc: 0.5379\n",
      "Epoch 12/60\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 1.3743 - acc: 0.5287 - val_loss: 1.2840 - val_acc: 0.5509\n",
      "Epoch 13/60\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 1.3535 - acc: 0.5343 - val_loss: 1.2477 - val_acc: 0.5594\n",
      "Epoch 14/60\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 1.3309 - acc: 0.5412 - val_loss: 1.2722 - val_acc: 0.5492\n",
      "Epoch 15/60\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 1.3213 - acc: 0.5477 - val_loss: 1.2488 - val_acc: 0.5589\n",
      "Epoch 16/60\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 1.3087 - acc: 0.5519 - val_loss: 1.3094 - val_acc: 0.5452\n",
      "Epoch 17/60\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 1.2983 - acc: 0.5581 - val_loss: 1.2641 - val_acc: 0.5588\n",
      "Epoch 18/60\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 1.2862 - acc: 0.5606 - val_loss: 1.2171 - val_acc: 0.5745\n",
      "Epoch 19/60\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 1.2691 - acc: 0.5672 - val_loss: 1.2550 - val_acc: 0.5602\n",
      "Epoch 20/60\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 1.2661 - acc: 0.5678 - val_loss: 1.1956 - val_acc: 0.5808\n",
      "Epoch 21/60\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 1.2503 - acc: 0.5719 - val_loss: 1.2668 - val_acc: 0.5534\n",
      "Epoch 22/60\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 1.2426 - acc: 0.5753 - val_loss: 1.2279 - val_acc: 0.5736\n",
      "Epoch 23/60\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 1.2255 - acc: 0.5821 - val_loss: 1.2332 - val_acc: 0.5660\n",
      "Epoch 24/60\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 1.2149 - acc: 0.5871 - val_loss: 1.2023 - val_acc: 0.5815\n",
      "Epoch 25/60\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 1.2068 - acc: 0.5900 - val_loss: 1.2170 - val_acc: 0.5734\n",
      "Epoch 26/60\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 1.1960 - acc: 0.5924 - val_loss: 1.2092 - val_acc: 0.5778\n",
      "Epoch 27/60\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 1.1921 - acc: 0.5959 - val_loss: 1.1802 - val_acc: 0.5852\n",
      "Epoch 28/60\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 1.1788 - acc: 0.5960 - val_loss: 1.2990 - val_acc: 0.5599\n",
      "Epoch 29/60\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 1.1781 - acc: 0.6012 - val_loss: 1.1832 - val_acc: 0.5842\n",
      "Epoch 30/60\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 1.1607 - acc: 0.6046 - val_loss: 1.2024 - val_acc: 0.5822\n",
      "Epoch 31/60\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 1.1484 - acc: 0.6106 - val_loss: 1.1957 - val_acc: 0.5802\n",
      "Epoch 32/60\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 1.1461 - acc: 0.6087 - val_loss: 1.1754 - val_acc: 0.5885\n",
      "Epoch 33/60\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 1.1363 - acc: 0.6121 - val_loss: 1.1677 - val_acc: 0.5944\n",
      "Epoch 34/60\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 1.1317 - acc: 0.6151 - val_loss: 1.1923 - val_acc: 0.5865\n",
      "Epoch 35/60\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 1.1222 - acc: 0.6181 - val_loss: 1.1557 - val_acc: 0.5939\n",
      "Epoch 36/60\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 1.1143 - acc: 0.6201 - val_loss: 1.1650 - val_acc: 0.5928\n",
      "Epoch 37/60\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 1.1085 - acc: 0.6224 - val_loss: 1.1709 - val_acc: 0.5937\n",
      "Epoch 38/60\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 1.1026 - acc: 0.6219 - val_loss: 1.1802 - val_acc: 0.5913\n",
      "Epoch 39/60\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 1.0913 - acc: 0.6301 - val_loss: 1.1753 - val_acc: 0.5928\n",
      "Epoch 40/60\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 1.0861 - acc: 0.6285 - val_loss: 1.1788 - val_acc: 0.5921\n",
      "Epoch 41/60\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 1.0833 - acc: 0.6319 - val_loss: 1.1685 - val_acc: 0.5870\n",
      "Epoch 42/60\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 1.0708 - acc: 0.6352 - val_loss: 1.1596 - val_acc: 0.5947\n",
      "Epoch 43/60\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 1.0656 - acc: 0.6374 - val_loss: 1.1485 - val_acc: 0.6011\n",
      "Epoch 44/60\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 1.0656 - acc: 0.6367 - val_loss: 1.1704 - val_acc: 0.5935\n",
      "Epoch 45/60\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 1.0533 - acc: 0.6402 - val_loss: 1.1818 - val_acc: 0.5940\n",
      "Epoch 46/60\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 1.0513 - acc: 0.6394 - val_loss: 1.1819 - val_acc: 0.5913\n",
      "Epoch 47/60\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 1.0472 - acc: 0.6446 - val_loss: 1.1471 - val_acc: 0.5986\n",
      "Epoch 48/60\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 1.0369 - acc: 0.6457 - val_loss: 1.1705 - val_acc: 0.5942\n",
      "Epoch 49/60\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 1.0314 - acc: 0.6470 - val_loss: 1.1826 - val_acc: 0.5906\n",
      "Epoch 50/60\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 1.0218 - acc: 0.6511 - val_loss: 1.1642 - val_acc: 0.5983\n",
      "Epoch 51/60\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 1.0132 - acc: 0.6536 - val_loss: 1.1871 - val_acc: 0.5971\n",
      "Epoch 52/60\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 1.0117 - acc: 0.6562 - val_loss: 1.1716 - val_acc: 0.5999\n",
      "Epoch 53/60\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 1.0057 - acc: 0.6558 - val_loss: 1.1715 - val_acc: 0.6049\n",
      "Epoch 54/60\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 0.9986 - acc: 0.6580 - val_loss: 1.1608 - val_acc: 0.6052\n",
      "Epoch 55/60\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.9948 - acc: 0.6634 - val_loss: 1.1982 - val_acc: 0.5902\n",
      "Epoch 56/60\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.9870 - acc: 0.6633 - val_loss: 1.1823 - val_acc: 0.5971\n",
      "Epoch 57/60\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.9832 - acc: 0.6658 - val_loss: 1.1860 - val_acc: 0.5985\n",
      "Epoch 58/60\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.9722 - acc: 0.6674 - val_loss: 1.1620 - val_acc: 0.6052\n",
      "Epoch 59/60\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.9703 - acc: 0.6690 - val_loss: 1.1652 - val_acc: 0.6078\n",
      "Epoch 60/60\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.9722 - acc: 0.6703 - val_loss: 1.1650 - val_acc: 0.6050\n"
     ]
    }
   ],
   "source": [
    "model_hist = model.fit(train_images, train_labels,\n",
    "                    epochs=60,\n",
    "                    batch_size=1024,\n",
    "                    validation_data=(validation_images, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "colab_type": "code",
    "id": "jB4aknytr0p7",
    "outputId": "d6a923b3-c412-445f-cfe3-4e584d95261a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEHCAYAAACjh0HiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLKOdNVV0ZQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "train_loss = model_hist.history['loss']\n",
    "val_loss= model_hist.history['val_loss']\n",
    "\n",
    "epochs_val = range(0, len(train_loss))\n",
    "epochs = range(0, len(train_loss))\n",
    "\n",
    "plt.plot(epochs_val, val_loss, 'b+', label='Validation', c = 'purple')\n",
    "plt.plot(epochs, train_loss, 'bo', label='training', c ='green')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8hNGDM-DnWB-"
   },
   "outputs": [],
   "source": [
    "model.save_weights('savedbase.h5')\n",
    "#od tego momentu - nowe eksperymenty\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=1e-4), \n",
    "              loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "FR48RWIkyKoH",
    "outputId": "91209965-e59f-4fea-ad50-db4376cdea8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "40000/40000 [==============================] - 5s 113us/sample - loss: 0.9277 - acc: 0.6839 - val_loss: 1.1174 - val_acc: 0.6191\n",
      "Epoch 2/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.9068 - acc: 0.6890 - val_loss: 1.1213 - val_acc: 0.6182\n",
      "Epoch 3/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.9018 - acc: 0.6921 - val_loss: 1.1257 - val_acc: 0.6196\n",
      "Epoch 4/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.9009 - acc: 0.6917 - val_loss: 1.1247 - val_acc: 0.6179\n",
      "Epoch 5/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.8943 - acc: 0.6968 - val_loss: 1.1268 - val_acc: 0.6181\n",
      "Epoch 6/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.8900 - acc: 0.6959 - val_loss: 1.1296 - val_acc: 0.6186\n",
      "Epoch 7/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.8976 - acc: 0.6942 - val_loss: 1.1248 - val_acc: 0.6195\n",
      "Epoch 8/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.8860 - acc: 0.6967 - val_loss: 1.1286 - val_acc: 0.6191\n",
      "Epoch 9/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.8905 - acc: 0.6957 - val_loss: 1.1278 - val_acc: 0.6194\n",
      "Epoch 10/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.8871 - acc: 0.6971 - val_loss: 1.1270 - val_acc: 0.6211\n",
      "Epoch 11/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.8836 - acc: 0.6989 - val_loss: 1.1288 - val_acc: 0.6208\n",
      "Epoch 12/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.8796 - acc: 0.6999 - val_loss: 1.1349 - val_acc: 0.6176\n",
      "Epoch 13/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.8756 - acc: 0.7003 - val_loss: 1.1308 - val_acc: 0.6180\n",
      "Epoch 14/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.8730 - acc: 0.7021 - val_loss: 1.1323 - val_acc: 0.6191\n",
      "Epoch 15/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.8756 - acc: 0.7028 - val_loss: 1.1302 - val_acc: 0.6205\n",
      "Epoch 16/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.8706 - acc: 0.7028 - val_loss: 1.1331 - val_acc: 0.6200\n",
      "Epoch 17/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.8751 - acc: 0.7026 - val_loss: 1.1317 - val_acc: 0.6197\n",
      "Epoch 18/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.8742 - acc: 0.7017 - val_loss: 1.1285 - val_acc: 0.6207\n",
      "Epoch 19/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.8678 - acc: 0.7028 - val_loss: 1.1345 - val_acc: 0.6194\n",
      "Epoch 20/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.8656 - acc: 0.7053 - val_loss: 1.1401 - val_acc: 0.6190\n",
      "Epoch 21/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.8657 - acc: 0.7046 - val_loss: 1.1323 - val_acc: 0.6201\n",
      "Epoch 22/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.8606 - acc: 0.7057 - val_loss: 1.1354 - val_acc: 0.6210\n",
      "Epoch 23/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.8630 - acc: 0.7074 - val_loss: 1.1403 - val_acc: 0.6213\n",
      "Epoch 24/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.8635 - acc: 0.7066 - val_loss: 1.1330 - val_acc: 0.6196\n",
      "Epoch 25/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.8590 - acc: 0.7050 - val_loss: 1.1393 - val_acc: 0.6187\n",
      "Epoch 26/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.8575 - acc: 0.7087 - val_loss: 1.1419 - val_acc: 0.6209\n",
      "Epoch 27/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.8574 - acc: 0.7086 - val_loss: 1.1361 - val_acc: 0.6230\n",
      "Epoch 28/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.8523 - acc: 0.7093 - val_loss: 1.1386 - val_acc: 0.6220\n",
      "Epoch 29/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.8514 - acc: 0.7078 - val_loss: 1.1383 - val_acc: 0.6209\n",
      "Epoch 30/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.8442 - acc: 0.7098 - val_loss: 1.1399 - val_acc: 0.6224\n",
      "Epoch 31/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.8492 - acc: 0.7099 - val_loss: 1.1421 - val_acc: 0.6201\n",
      "Epoch 32/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.8460 - acc: 0.7136 - val_loss: 1.1430 - val_acc: 0.6191\n",
      "Epoch 33/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.8457 - acc: 0.7111 - val_loss: 1.1433 - val_acc: 0.6206\n",
      "Epoch 34/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.8404 - acc: 0.7147 - val_loss: 1.1398 - val_acc: 0.6223\n",
      "Epoch 35/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.8409 - acc: 0.7122 - val_loss: 1.1414 - val_acc: 0.6214\n",
      "Epoch 36/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.8371 - acc: 0.7146 - val_loss: 1.1423 - val_acc: 0.6238\n",
      "Epoch 37/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.8274 - acc: 0.7186 - val_loss: 1.1423 - val_acc: 0.6201\n",
      "Epoch 38/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.8371 - acc: 0.7152 - val_loss: 1.1401 - val_acc: 0.6228\n",
      "Epoch 39/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.8385 - acc: 0.7110 - val_loss: 1.1430 - val_acc: 0.6216\n",
      "Epoch 40/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.8306 - acc: 0.7171 - val_loss: 1.1422 - val_acc: 0.6225\n",
      "Epoch 41/50\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 0.8287 - acc: 0.7161 - val_loss: 1.1484 - val_acc: 0.6202\n",
      "Epoch 42/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.8315 - acc: 0.7175 - val_loss: 1.1457 - val_acc: 0.6201\n",
      "Epoch 43/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.8308 - acc: 0.7157 - val_loss: 1.1461 - val_acc: 0.6221\n",
      "Epoch 44/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.8273 - acc: 0.7168 - val_loss: 1.1424 - val_acc: 0.6241\n",
      "Epoch 45/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.8253 - acc: 0.7187 - val_loss: 1.1466 - val_acc: 0.6223\n",
      "Epoch 46/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.8215 - acc: 0.7211 - val_loss: 1.1521 - val_acc: 0.6207\n",
      "Epoch 47/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.8212 - acc: 0.7199 - val_loss: 1.1467 - val_acc: 0.6217\n",
      "Epoch 48/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.8211 - acc: 0.7176 - val_loss: 1.1468 - val_acc: 0.6235\n",
      "Epoch 49/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.8270 - acc: 0.7190 - val_loss: 1.1450 - val_acc: 0.6223\n",
      "Epoch 50/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.8181 - acc: 0.7228 - val_loss: 1.1526 - val_acc: 0.6239\n"
     ]
    }
   ],
   "source": [
    "conv_base.trainable = True\n",
    "set_trainable = False\n",
    "\n",
    "for layer in conv_base.layers:\n",
    "    if layer.name == 'block5_conv1' or layer.name == 'block5_conv2' or layer.name == 'block5_conv3' :\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "\n",
    "model_hist = model.fit(train_images, train_labels,\n",
    "                    epochs=50,\n",
    "                    batch_size=1024,\n",
    "                    validation_data=(validation_images, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "XIy4ctAJ_5g7",
    "outputId": "cc7aa927-4110-4e43-bb3d-5ec1b2b0fe63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.8162 - acc: 0.7198 - val_loss: 1.1479 - val_acc: 0.6211\n",
      "Epoch 2/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.8140 - acc: 0.7212 - val_loss: 1.1573 - val_acc: 0.6217\n",
      "Epoch 3/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.8192 - acc: 0.7215 - val_loss: 1.1498 - val_acc: 0.6226\n",
      "Epoch 4/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.8120 - acc: 0.7212 - val_loss: 1.1517 - val_acc: 0.6249\n",
      "Epoch 5/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.8097 - acc: 0.7243 - val_loss: 1.1551 - val_acc: 0.6195\n",
      "Epoch 6/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.8088 - acc: 0.7249 - val_loss: 1.1532 - val_acc: 0.6205\n",
      "Epoch 7/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.8105 - acc: 0.7229 - val_loss: 1.1511 - val_acc: 0.6202\n",
      "Epoch 8/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.8080 - acc: 0.7239 - val_loss: 1.1549 - val_acc: 0.6232\n",
      "Epoch 9/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.8054 - acc: 0.7241 - val_loss: 1.1535 - val_acc: 0.6240\n",
      "Epoch 10/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.8027 - acc: 0.7250 - val_loss: 1.1538 - val_acc: 0.6222\n",
      "Epoch 11/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.7997 - acc: 0.7265 - val_loss: 1.1566 - val_acc: 0.6219\n",
      "Epoch 12/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.8008 - acc: 0.7254 - val_loss: 1.1589 - val_acc: 0.6222\n",
      "Epoch 13/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.8038 - acc: 0.7255 - val_loss: 1.1564 - val_acc: 0.6236\n",
      "Epoch 14/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.8000 - acc: 0.7282 - val_loss: 1.1608 - val_acc: 0.6204\n",
      "Epoch 15/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.7946 - acc: 0.7290 - val_loss: 1.1575 - val_acc: 0.6216\n",
      "Epoch 16/50\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 0.7945 - acc: 0.7300 - val_loss: 1.1589 - val_acc: 0.6232\n",
      "Epoch 17/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.7922 - acc: 0.7301 - val_loss: 1.1551 - val_acc: 0.6237\n",
      "Epoch 18/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7940 - acc: 0.7286 - val_loss: 1.1609 - val_acc: 0.6216\n",
      "Epoch 19/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7977 - acc: 0.7267 - val_loss: 1.1598 - val_acc: 0.6229\n",
      "Epoch 20/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.7918 - acc: 0.7301 - val_loss: 1.1570 - val_acc: 0.6230\n",
      "Epoch 21/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7884 - acc: 0.7311 - val_loss: 1.1624 - val_acc: 0.6227\n",
      "Epoch 22/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7899 - acc: 0.7296 - val_loss: 1.1622 - val_acc: 0.6212\n",
      "Epoch 23/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7868 - acc: 0.7329 - val_loss: 1.1616 - val_acc: 0.6263\n",
      "Epoch 24/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7804 - acc: 0.7341 - val_loss: 1.1609 - val_acc: 0.6218\n",
      "Epoch 25/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7826 - acc: 0.7317 - val_loss: 1.1657 - val_acc: 0.6207\n",
      "Epoch 26/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7803 - acc: 0.7334 - val_loss: 1.1607 - val_acc: 0.6229\n",
      "Epoch 27/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7826 - acc: 0.7314 - val_loss: 1.1620 - val_acc: 0.6229\n",
      "Epoch 28/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.7818 - acc: 0.7315 - val_loss: 1.1656 - val_acc: 0.6209\n",
      "Epoch 29/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7778 - acc: 0.7350 - val_loss: 1.1653 - val_acc: 0.6227\n",
      "Epoch 30/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7747 - acc: 0.7336 - val_loss: 1.1672 - val_acc: 0.6225\n",
      "Epoch 31/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.7790 - acc: 0.7343 - val_loss: 1.1682 - val_acc: 0.6230\n",
      "Epoch 32/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7782 - acc: 0.7344 - val_loss: 1.1608 - val_acc: 0.6227\n",
      "Epoch 33/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7716 - acc: 0.7341 - val_loss: 1.1660 - val_acc: 0.6209\n",
      "Epoch 34/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.7798 - acc: 0.7331 - val_loss: 1.1662 - val_acc: 0.6211\n",
      "Epoch 35/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.7768 - acc: 0.7350 - val_loss: 1.1654 - val_acc: 0.6244\n",
      "Epoch 36/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.7624 - acc: 0.7391 - val_loss: 1.1717 - val_acc: 0.6226\n",
      "Epoch 37/50\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 0.7696 - acc: 0.7355 - val_loss: 1.1651 - val_acc: 0.6230\n",
      "Epoch 38/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7708 - acc: 0.7359 - val_loss: 1.1741 - val_acc: 0.6207\n",
      "Epoch 39/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.7668 - acc: 0.7391 - val_loss: 1.1703 - val_acc: 0.6246\n",
      "Epoch 40/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7662 - acc: 0.7398 - val_loss: 1.1694 - val_acc: 0.6211\n",
      "Epoch 41/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.7682 - acc: 0.7370 - val_loss: 1.1691 - val_acc: 0.6242\n",
      "Epoch 42/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.7572 - acc: 0.7404 - val_loss: 1.1727 - val_acc: 0.6219\n",
      "Epoch 43/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7620 - acc: 0.7413 - val_loss: 1.1747 - val_acc: 0.6234\n",
      "Epoch 44/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7672 - acc: 0.7397 - val_loss: 1.1779 - val_acc: 0.6201\n",
      "Epoch 45/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.7615 - acc: 0.7389 - val_loss: 1.1766 - val_acc: 0.6242\n",
      "Epoch 46/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.7571 - acc: 0.7400 - val_loss: 1.1722 - val_acc: 0.6230\n",
      "Epoch 47/50\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 0.7577 - acc: 0.7398 - val_loss: 1.1723 - val_acc: 0.6202\n",
      "Epoch 48/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.7562 - acc: 0.7410 - val_loss: 1.1774 - val_acc: 0.6236\n",
      "Epoch 49/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7568 - acc: 0.7398 - val_loss: 1.1760 - val_acc: 0.6222\n",
      "Epoch 50/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7541 - acc: 0.7420 - val_loss: 1.1834 - val_acc: 0.6205\n"
     ]
    }
   ],
   "source": [
    "conv_base.trainable = True\n",
    "set_trainable = False\n",
    "\n",
    "for layer in conv_base.layers:\n",
    "    if layer.name == 'block4_conv1' or layer.name == 'block4_conv2' or layer.name == 'block4_conv3' :\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "\n",
    "model_hist = model.fit(train_images, train_labels,\n",
    "                    epochs=50,\n",
    "                    batch_size=1024,\n",
    "                    validation_data=(validation_images, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "QnsaiIupwWZR",
    "outputId": "1dacac27-9ea9-4576-ac41-28ec2bbe96cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.7495 - acc: 0.7428 - val_loss: 1.1775 - val_acc: 0.6245\n",
      "Epoch 2/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7475 - acc: 0.7437 - val_loss: 1.1839 - val_acc: 0.6211\n",
      "Epoch 3/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7505 - acc: 0.7418 - val_loss: 1.1801 - val_acc: 0.6232\n",
      "Epoch 4/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.7494 - acc: 0.7452 - val_loss: 1.1762 - val_acc: 0.6240\n",
      "Epoch 5/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7479 - acc: 0.7442 - val_loss: 1.1790 - val_acc: 0.6226\n",
      "Epoch 6/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7456 - acc: 0.7477 - val_loss: 1.1841 - val_acc: 0.6209\n",
      "Epoch 7/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.7488 - acc: 0.7452 - val_loss: 1.1839 - val_acc: 0.6229\n",
      "Epoch 8/50\n",
      "40000/40000 [==============================] - 4s 102us/sample - loss: 0.7450 - acc: 0.7441 - val_loss: 1.1815 - val_acc: 0.6224\n",
      "Epoch 9/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.7456 - acc: 0.7465 - val_loss: 1.1831 - val_acc: 0.6244\n",
      "Epoch 10/50\n",
      "40000/40000 [==============================] - 4s 102us/sample - loss: 0.7386 - acc: 0.7474 - val_loss: 1.1875 - val_acc: 0.6201\n",
      "Epoch 11/50\n",
      "40000/40000 [==============================] - 4s 102us/sample - loss: 0.7453 - acc: 0.7449 - val_loss: 1.1865 - val_acc: 0.6218\n",
      "Epoch 12/50\n",
      "40000/40000 [==============================] - 4s 102us/sample - loss: 0.7376 - acc: 0.7496 - val_loss: 1.1833 - val_acc: 0.6237\n",
      "Epoch 13/50\n",
      "40000/40000 [==============================] - 4s 102us/sample - loss: 0.7360 - acc: 0.7467 - val_loss: 1.1850 - val_acc: 0.6228\n",
      "Epoch 14/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7347 - acc: 0.7489 - val_loss: 1.1872 - val_acc: 0.6218\n",
      "Epoch 15/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.7336 - acc: 0.7507 - val_loss: 1.1886 - val_acc: 0.6226\n",
      "Epoch 16/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7388 - acc: 0.7487 - val_loss: 1.1920 - val_acc: 0.6202\n",
      "Epoch 17/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7269 - acc: 0.7528 - val_loss: 1.1884 - val_acc: 0.6238\n",
      "Epoch 18/50\n",
      "40000/40000 [==============================] - 4s 102us/sample - loss: 0.7339 - acc: 0.7494 - val_loss: 1.1893 - val_acc: 0.6222\n",
      "Epoch 19/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7329 - acc: 0.7501 - val_loss: 1.1848 - val_acc: 0.6214\n",
      "Epoch 20/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7285 - acc: 0.7494 - val_loss: 1.1923 - val_acc: 0.6232\n",
      "Epoch 21/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7193 - acc: 0.7526 - val_loss: 1.1925 - val_acc: 0.6213\n",
      "Epoch 22/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7274 - acc: 0.7492 - val_loss: 1.1936 - val_acc: 0.6210\n",
      "Epoch 23/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7258 - acc: 0.7503 - val_loss: 1.1995 - val_acc: 0.6209\n",
      "Epoch 24/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7285 - acc: 0.7497 - val_loss: 1.1923 - val_acc: 0.6220\n",
      "Epoch 25/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7227 - acc: 0.7509 - val_loss: 1.1935 - val_acc: 0.6218\n",
      "Epoch 26/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7273 - acc: 0.7495 - val_loss: 1.1921 - val_acc: 0.6222\n",
      "Epoch 27/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7194 - acc: 0.7538 - val_loss: 1.1984 - val_acc: 0.6222\n",
      "Epoch 28/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.7222 - acc: 0.7546 - val_loss: 1.1934 - val_acc: 0.6210\n",
      "Epoch 29/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7166 - acc: 0.7551 - val_loss: 1.1953 - val_acc: 0.6248\n",
      "Epoch 30/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7213 - acc: 0.7549 - val_loss: 1.1925 - val_acc: 0.6236\n",
      "Epoch 31/50\n",
      "40000/40000 [==============================] - 4s 102us/sample - loss: 0.7195 - acc: 0.7557 - val_loss: 1.1962 - val_acc: 0.6223\n",
      "Epoch 32/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7148 - acc: 0.7553 - val_loss: 1.1961 - val_acc: 0.6223\n",
      "Epoch 33/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7174 - acc: 0.7556 - val_loss: 1.1940 - val_acc: 0.6235\n",
      "Epoch 34/50\n",
      "40000/40000 [==============================] - 4s 102us/sample - loss: 0.7177 - acc: 0.7537 - val_loss: 1.1933 - val_acc: 0.6226\n",
      "Epoch 35/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7166 - acc: 0.7541 - val_loss: 1.1999 - val_acc: 0.6223\n",
      "Epoch 36/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7090 - acc: 0.7578 - val_loss: 1.1986 - val_acc: 0.6235\n",
      "Epoch 37/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.7115 - acc: 0.7557 - val_loss: 1.2059 - val_acc: 0.6219\n",
      "Epoch 38/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7033 - acc: 0.7582 - val_loss: 1.2008 - val_acc: 0.6216\n",
      "Epoch 39/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7122 - acc: 0.7555 - val_loss: 1.2010 - val_acc: 0.6238\n",
      "Epoch 40/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7043 - acc: 0.7588 - val_loss: 1.2015 - val_acc: 0.6212\n",
      "Epoch 41/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7038 - acc: 0.7588 - val_loss: 1.2023 - val_acc: 0.6206\n",
      "Epoch 42/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7008 - acc: 0.7598 - val_loss: 1.2075 - val_acc: 0.6209\n",
      "Epoch 43/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7087 - acc: 0.7575 - val_loss: 1.2082 - val_acc: 0.6215\n",
      "Epoch 44/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.7006 - acc: 0.7599 - val_loss: 1.2043 - val_acc: 0.6224\n",
      "Epoch 45/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7029 - acc: 0.7588 - val_loss: 1.2083 - val_acc: 0.6218\n",
      "Epoch 46/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7016 - acc: 0.7584 - val_loss: 1.2078 - val_acc: 0.6235\n",
      "Epoch 47/50\n",
      "40000/40000 [==============================] - 4s 102us/sample - loss: 0.6990 - acc: 0.7603 - val_loss: 1.2022 - val_acc: 0.6233\n",
      "Epoch 48/50\n",
      "40000/40000 [==============================] - 4s 102us/sample - loss: 0.6918 - acc: 0.7635 - val_loss: 1.2056 - val_acc: 0.6229\n",
      "Epoch 49/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.7079 - acc: 0.7590 - val_loss: 1.2083 - val_acc: 0.6232\n",
      "Epoch 50/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.6989 - acc: 0.7617 - val_loss: 1.2116 - val_acc: 0.6228\n"
     ]
    }
   ],
   "source": [
    "conv_base.trainable = True\n",
    "set_trainable = False\n",
    "\n",
    "for layer in conv_base.layers:\n",
    "    if layer.name == 'block5_conv1' or layer.name == 'block5_conv2' or layer.name == 'block5_conv3' :\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "\n",
    "model_hist = model.fit(train_images, train_labels,\n",
    "                    epochs=50,\n",
    "                    batch_size=1024,\n",
    "                    validation_data=(validation_images, validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZkRRMzjV88QM"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "## Final ...\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "YBE1wmcay3Dp",
    "outputId": "149fc805-a151-4e7d-89af-6be7892f2667"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "40000/40000 [==============================] - 4s 107us/sample - loss: 0.7004 - acc: 0.7611 - val_loss: 1.2126 - val_acc: 0.6202\n",
      "Epoch 2/50\n",
      "40000/40000 [==============================] - 4s 106us/sample - loss: 0.6914 - acc: 0.7617 - val_loss: 1.2150 - val_acc: 0.6222\n",
      "Epoch 3/50\n",
      "40000/40000 [==============================] - 4s 106us/sample - loss: 0.6919 - acc: 0.7624 - val_loss: 1.2145 - val_acc: 0.6207\n",
      "Epoch 4/50\n",
      "40000/40000 [==============================] - 4s 106us/sample - loss: 0.6901 - acc: 0.7659 - val_loss: 1.2127 - val_acc: 0.6197\n",
      "Epoch 5/50\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 0.6985 - acc: 0.7633 - val_loss: 1.2134 - val_acc: 0.6186\n",
      "Epoch 6/50\n",
      "40000/40000 [==============================] - 4s 106us/sample - loss: 0.6877 - acc: 0.7667 - val_loss: 1.2175 - val_acc: 0.6207\n",
      "Epoch 7/50\n",
      "40000/40000 [==============================] - 4s 106us/sample - loss: 0.6900 - acc: 0.7628 - val_loss: 1.2082 - val_acc: 0.6214\n",
      "Epoch 8/50\n",
      "40000/40000 [==============================] - 4s 106us/sample - loss: 0.6884 - acc: 0.7648 - val_loss: 1.2170 - val_acc: 0.6208\n",
      "Epoch 9/50\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 0.6818 - acc: 0.7652 - val_loss: 1.2140 - val_acc: 0.6185\n",
      "Epoch 10/50\n",
      "40000/40000 [==============================] - 4s 106us/sample - loss: 0.6862 - acc: 0.7639 - val_loss: 1.2233 - val_acc: 0.6188\n",
      "Epoch 11/50\n",
      "40000/40000 [==============================] - 4s 106us/sample - loss: 0.6856 - acc: 0.7637 - val_loss: 1.2221 - val_acc: 0.6237\n",
      "Epoch 12/50\n",
      "40000/40000 [==============================] - 4s 106us/sample - loss: 0.6857 - acc: 0.7641 - val_loss: 1.2194 - val_acc: 0.6209\n",
      "Epoch 13/50\n",
      "40000/40000 [==============================] - 4s 106us/sample - loss: 0.6875 - acc: 0.7641 - val_loss: 1.2186 - val_acc: 0.6248\n",
      "Epoch 14/50\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 0.6821 - acc: 0.7664 - val_loss: 1.2216 - val_acc: 0.6207\n",
      "Epoch 15/50\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 0.6798 - acc: 0.7690 - val_loss: 1.2214 - val_acc: 0.6231\n",
      "Epoch 16/50\n",
      "40000/40000 [==============================] - 4s 106us/sample - loss: 0.6847 - acc: 0.7642 - val_loss: 1.2134 - val_acc: 0.6230\n",
      "Epoch 17/50\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 0.6755 - acc: 0.7673 - val_loss: 1.2198 - val_acc: 0.6222\n",
      "Epoch 18/50\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 0.6756 - acc: 0.7689 - val_loss: 1.2217 - val_acc: 0.6189\n",
      "Epoch 19/50\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 0.6731 - acc: 0.7697 - val_loss: 1.2230 - val_acc: 0.6218\n",
      "Epoch 20/50\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 0.6790 - acc: 0.7664 - val_loss: 1.2190 - val_acc: 0.6212\n",
      "Epoch 21/50\n",
      "40000/40000 [==============================] - 4s 105us/sample - loss: 0.6743 - acc: 0.7691 - val_loss: 1.2217 - val_acc: 0.6210\n",
      "Epoch 22/50\n",
      "40000/40000 [==============================] - 4s 106us/sample - loss: 0.6765 - acc: 0.7685 - val_loss: 1.2228 - val_acc: 0.6215\n",
      "Epoch 23/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.6713 - acc: 0.7696 - val_loss: 1.2314 - val_acc: 0.6227\n",
      "Epoch 24/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.6638 - acc: 0.7727 - val_loss: 1.2279 - val_acc: 0.6211\n",
      "Epoch 25/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.6699 - acc: 0.7721 - val_loss: 1.2335 - val_acc: 0.6199\n",
      "Epoch 26/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.6679 - acc: 0.7714 - val_loss: 1.2306 - val_acc: 0.6219\n",
      "Epoch 27/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.6702 - acc: 0.7709 - val_loss: 1.2294 - val_acc: 0.6223\n",
      "Epoch 28/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.6676 - acc: 0.7721 - val_loss: 1.2340 - val_acc: 0.6197\n",
      "Epoch 29/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.6620 - acc: 0.7747 - val_loss: 1.2300 - val_acc: 0.6215\n",
      "Epoch 30/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.6679 - acc: 0.7694 - val_loss: 1.2361 - val_acc: 0.6199\n",
      "Epoch 31/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.6704 - acc: 0.7710 - val_loss: 1.2259 - val_acc: 0.6233\n",
      "Epoch 32/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.6651 - acc: 0.7715 - val_loss: 1.2342 - val_acc: 0.6210\n",
      "Epoch 33/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.6705 - acc: 0.7705 - val_loss: 1.2305 - val_acc: 0.6212\n",
      "Epoch 34/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.6591 - acc: 0.7731 - val_loss: 1.2315 - val_acc: 0.6223\n",
      "Epoch 35/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.6635 - acc: 0.7722 - val_loss: 1.2352 - val_acc: 0.6222\n",
      "Epoch 36/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.6586 - acc: 0.7742 - val_loss: 1.2390 - val_acc: 0.6221\n",
      "Epoch 37/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.6566 - acc: 0.7741 - val_loss: 1.2393 - val_acc: 0.6221\n",
      "Epoch 38/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.6604 - acc: 0.7740 - val_loss: 1.2338 - val_acc: 0.6220\n",
      "Epoch 39/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.6613 - acc: 0.7732 - val_loss: 1.2368 - val_acc: 0.6235\n",
      "Epoch 40/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.6580 - acc: 0.7761 - val_loss: 1.2379 - val_acc: 0.6218\n",
      "Epoch 41/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.6595 - acc: 0.7764 - val_loss: 1.2359 - val_acc: 0.6216\n",
      "Epoch 42/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.6556 - acc: 0.7749 - val_loss: 1.2483 - val_acc: 0.6199\n",
      "Epoch 43/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.6545 - acc: 0.7753 - val_loss: 1.2352 - val_acc: 0.6254\n",
      "Epoch 44/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.6549 - acc: 0.7739 - val_loss: 1.2439 - val_acc: 0.6212\n",
      "Epoch 45/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.6607 - acc: 0.7750 - val_loss: 1.2366 - val_acc: 0.6230\n",
      "Epoch 46/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.6575 - acc: 0.7754 - val_loss: 1.2365 - val_acc: 0.6239\n",
      "Epoch 47/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.6534 - acc: 0.7796 - val_loss: 1.2454 - val_acc: 0.6223\n",
      "Epoch 48/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.6517 - acc: 0.7775 - val_loss: 1.2363 - val_acc: 0.6219\n",
      "Epoch 49/50\n",
      "40000/40000 [==============================] - 4s 104us/sample - loss: 0.6475 - acc: 0.7786 - val_loss: 1.2456 - val_acc: 0.6217\n",
      "Epoch 50/50\n",
      "40000/40000 [==============================] - 4s 103us/sample - loss: 0.6450 - acc: 0.7789 - val_loss: 1.2471 - val_acc: 0.6235\n"
     ]
    }
   ],
   "source": [
    "conv_base.trainable = False\n",
    "\n",
    "model_hist = model.fit(train_images, train_labels,\n",
    "                    epochs=50,\n",
    "                    batch_size=1024,\n",
    "                    validation_data=(validation_images, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fume-mRR9EKn"
   },
   "outputs": [],
   "source": [
    "model.save_weights('final_tuning.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "KWoVxF67zBeC",
    "outputId": "c994f7a3-eaa8-4c8a-fed4-7e58ab524519"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEoGISMYpEYiIZJwSgYhIxikRiIhk3P8HgISxj2/vYBQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "train_loss = model_hist.history['loss']\n",
    "val_loss= model_hist.history['val_loss']\n",
    "\n",
    "epochs_val = range(0, len(train_loss))\n",
    "epochs = range(0, len(train_loss))\n",
    "\n",
    "plt.plot(epochs_val, val_loss, 'b+', label='Validation', c = 'purple')\n",
    "plt.plot(epochs, train_loss, 'bo', label='training', c ='green')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v2w5EM9pMpk4"
   },
   "outputs": [],
   "source": [
    "model.load_weights('final_tuning.h5')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('test_acc:', test_acc)\n",
    "test_loss"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "VGG.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
