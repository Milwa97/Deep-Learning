{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98
    },
    "colab_type": "code",
    "id": "INABZhVTnEem",
    "outputId": "b2870770-9f01-426b-c050-0b61dd872e23"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2.2.5'"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "colab_type": "code",
    "id": "7bCPEzw0nNGo",
    "outputId": "b3312521-ed54-4142-de7a-4966f530b28b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2R2AyL_cnVYQ"
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras.models as models\n",
    "import tensorflow.keras.layers as layers \n",
    "import tensorflow.keras.optimizers as optimizers\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "n2PK5q1EnkI6",
    "outputId": "3b0995f2-01d5-47e6-d6a7-563b76b20234"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 6s 0us/step\n",
      "Training set: images (40000, 32, 32, 3)\tlabels: (40000, 1)\n",
      "validation set: images (10000, 32, 32, 3)\tlabels: (10000, 1)\n",
      "Test set: images (10000, 32, 32, 3)\tlabels: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "\n",
    "validation_images = train_images[40000:]\n",
    "validation_labels = train_labels[40000:]\n",
    "\n",
    "train_images = train_images[:40000]\n",
    "train_labels = train_labels[:40000]\n",
    "\n",
    "# CHECKER:\n",
    "\n",
    "print(\"Training set: images {:}\\tlabels: {:}\".format(train_images.shape, train_labels.shape ) )\n",
    "print(\"validation set: images {:}\\tlabels: {:}\".format(validation_images.shape, validation_labels.shape ) )\n",
    "print(\"Test set: images {:}\\tlabels: {:}\".format(test_images.shape, test_labels.shape ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "1oi4w96xn0K_",
    "outputId": "e7e01515-bcf1-45d5-a9f9-202a3e89cb57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: images (40000, 32, 32, 3)\tlabels: (40000, 10)\n",
      "validation set: images (10000, 32, 32, 3)\tlabels: (10000, 10)\n",
      "Test set: images (10000, 32, 32, 3)\tlabels: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "train_images = train_images.reshape((40000, 32, 32, 3))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "validation_images = validation_images.reshape((10000, 32, 32, 3))\n",
    "validation_images = validation_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 32, 32, 3))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "validation_labels = to_categorical(validation_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "\n",
    "print(\"Training set: images {:}\\tlabels: {:}\".format(train_images.shape, train_labels.shape ) )\n",
    "print(\"validation set: images {:}\\tlabels: {:}\".format(validation_images.shape, validation_labels.shape ) )\n",
    "print(\"Test set: images {:}\\tlabels: {:}\".format(test_images.shape, test_labels.shape ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "OqjEz5aIn3jI",
    "outputId": "6df2c803-4600-4f06-9bf4-7215f607bed6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "80142336/80134624 [==============================] - 3s 0us/step\n",
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 20,024,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG19 \n",
    "\n",
    "conv_base = VGG19(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(32, 32, 3))\n",
    "\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "TKIB4IsTn7jH",
    "outputId": "f1e36c0d-05ec-4862-c9ae-b980ad172586"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train features: (40000, 1, 1, 512)\n",
      "validation features: (10000, 1, 1, 512)\n",
      "test features: (10000, 1, 1, 512)\n"
     ]
    }
   ],
   "source": [
    "def extract_features(input_data):\n",
    "    return conv_base.predict(input_data) \n",
    "\n",
    "train_features = extract_features(train_images)\n",
    "validation_features = extract_features(validation_images)\n",
    "test_features = extract_features(test_images)\n",
    "\n",
    "print(\"train features:\", train_features.shape)\n",
    "print(\"validation features:\", validation_features.shape)\n",
    "print(\"test features:\", test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "00olRTmxn_i5",
    "outputId": "6f68f7b4-a9d8-491f-f768-8dd2535612ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train features: (40000, 512)\n",
      "validation features: (10000, 512)\n",
      "test features: (10000, 512)\n"
     ]
    }
   ],
   "source": [
    "train_features = np.reshape(train_features, (len(train_labels) , 512))\n",
    "validation_features = np.reshape(validation_features, (len(validation_labels),512))\n",
    "test_features = np.reshape(test_features, (len(test_labels), 512))\n",
    "\n",
    "print(\"train features:\", train_features.shape)\n",
    "print(\"validation features:\", validation_features.shape)\n",
    "print(\"test features:\", test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "26iEEcGkoHLo",
    "outputId": "2a1d1c67-3e02-4b25-9d26-fa9c50711c55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "40000/40000 [==============================] - 0s 12us/sample - loss: 2.4690 - acc: 0.1032 - val_loss: 2.3020 - val_acc: 0.1171\n",
      "Epoch 2/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 2.3472 - acc: 0.1012 - val_loss: 2.3011 - val_acc: 0.1167\n",
      "Epoch 3/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 2.3216 - acc: 0.1040 - val_loss: 2.3013 - val_acc: 0.0955\n",
      "Epoch 4/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 2.3119 - acc: 0.1033 - val_loss: 2.3013 - val_acc: 0.0878\n",
      "Epoch 5/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 2.3082 - acc: 0.1059 - val_loss: 2.3010 - val_acc: 0.0877\n",
      "Epoch 6/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 2.3030 - acc: 0.1099 - val_loss: 2.3001 - val_acc: 0.1015\n",
      "Epoch 7/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 2.2991 - acc: 0.1149 - val_loss: 2.2973 - val_acc: 0.1278\n",
      "Epoch 8/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 2.2970 - acc: 0.1149 - val_loss: 2.2935 - val_acc: 0.1454\n",
      "Epoch 9/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 2.2922 - acc: 0.1187 - val_loss: 2.2889 - val_acc: 0.1574\n",
      "Epoch 10/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 2.2881 - acc: 0.1212 - val_loss: 2.2807 - val_acc: 0.1754\n",
      "Epoch 11/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 2.2782 - acc: 0.1348 - val_loss: 2.2673 - val_acc: 0.1919\n",
      "Epoch 12/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 2.2721 - acc: 0.1356 - val_loss: 2.2542 - val_acc: 0.2183\n",
      "Epoch 13/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 2.2630 - acc: 0.1430 - val_loss: 2.2407 - val_acc: 0.2222\n",
      "Epoch 14/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 2.2532 - acc: 0.1515 - val_loss: 2.2264 - val_acc: 0.2181\n",
      "Epoch 15/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 2.2454 - acc: 0.1526 - val_loss: 2.2111 - val_acc: 0.2158\n",
      "Epoch 16/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 2.2315 - acc: 0.1586 - val_loss: 2.1952 - val_acc: 0.2212\n",
      "Epoch 17/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 2.2236 - acc: 0.1619 - val_loss: 2.1815 - val_acc: 0.2259\n",
      "Epoch 18/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 2.2129 - acc: 0.1705 - val_loss: 2.1573 - val_acc: 0.2346\n",
      "Epoch 19/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 2.1999 - acc: 0.1781 - val_loss: 2.1483 - val_acc: 0.2434\n",
      "Epoch 20/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 2.1906 - acc: 0.1792 - val_loss: 2.1303 - val_acc: 0.2497\n",
      "Epoch 21/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 2.1777 - acc: 0.1865 - val_loss: 2.1036 - val_acc: 0.2582\n",
      "Epoch 22/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 2.1656 - acc: 0.1950 - val_loss: 2.0899 - val_acc: 0.2716\n",
      "Epoch 23/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 2.1561 - acc: 0.1955 - val_loss: 2.0733 - val_acc: 0.2854\n",
      "Epoch 24/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 2.1422 - acc: 0.2046 - val_loss: 2.0550 - val_acc: 0.2964\n",
      "Epoch 25/200\n",
      "40000/40000 [==============================] - 0s 5us/sample - loss: 2.1332 - acc: 0.2063 - val_loss: 2.0429 - val_acc: 0.3025\n",
      "Epoch 26/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 2.1203 - acc: 0.2163 - val_loss: 2.0211 - val_acc: 0.3159\n",
      "Epoch 27/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 2.1069 - acc: 0.2232 - val_loss: 2.0039 - val_acc: 0.3246\n",
      "Epoch 28/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 2.0970 - acc: 0.2283 - val_loss: 1.9792 - val_acc: 0.3302\n",
      "Epoch 29/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 2.0862 - acc: 0.2287 - val_loss: 1.9544 - val_acc: 0.3338\n",
      "Epoch 30/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 2.0720 - acc: 0.2380 - val_loss: 1.9449 - val_acc: 0.3408\n",
      "Epoch 31/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 2.0598 - acc: 0.2430 - val_loss: 1.9214 - val_acc: 0.3391\n",
      "Epoch 32/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 2.0434 - acc: 0.2509 - val_loss: 1.8956 - val_acc: 0.3432\n",
      "Epoch 33/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 2.0362 - acc: 0.2524 - val_loss: 1.8811 - val_acc: 0.3499\n",
      "Epoch 34/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 2.0178 - acc: 0.2621 - val_loss: 1.8586 - val_acc: 0.3573\n",
      "Epoch 35/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 2.0078 - acc: 0.2654 - val_loss: 1.8441 - val_acc: 0.3663\n",
      "Epoch 36/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.9994 - acc: 0.2704 - val_loss: 1.8320 - val_acc: 0.3735\n",
      "Epoch 37/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.9832 - acc: 0.2748 - val_loss: 1.7973 - val_acc: 0.3768\n",
      "Epoch 38/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.9750 - acc: 0.2793 - val_loss: 1.7937 - val_acc: 0.3886\n",
      "Epoch 39/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.9571 - acc: 0.2858 - val_loss: 1.7736 - val_acc: 0.3921\n",
      "Epoch 40/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.9429 - acc: 0.2871 - val_loss: 1.7585 - val_acc: 0.3952\n",
      "Epoch 41/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.9386 - acc: 0.2939 - val_loss: 1.7429 - val_acc: 0.3996\n",
      "Epoch 42/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.9264 - acc: 0.2986 - val_loss: 1.7274 - val_acc: 0.4036\n",
      "Epoch 43/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.9163 - acc: 0.3057 - val_loss: 1.7105 - val_acc: 0.4126\n",
      "Epoch 44/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.9068 - acc: 0.3057 - val_loss: 1.7050 - val_acc: 0.4161\n",
      "Epoch 45/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.8965 - acc: 0.3097 - val_loss: 1.6874 - val_acc: 0.4189\n",
      "Epoch 46/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.8823 - acc: 0.3135 - val_loss: 1.6726 - val_acc: 0.4222\n",
      "Epoch 47/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.8759 - acc: 0.3169 - val_loss: 1.6611 - val_acc: 0.4278\n",
      "Epoch 48/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.8662 - acc: 0.3202 - val_loss: 1.6457 - val_acc: 0.4290\n",
      "Epoch 49/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.8530 - acc: 0.3268 - val_loss: 1.6397 - val_acc: 0.4302\n",
      "Epoch 50/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.8416 - acc: 0.3275 - val_loss: 1.6197 - val_acc: 0.4354\n",
      "Epoch 51/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.8371 - acc: 0.3290 - val_loss: 1.6213 - val_acc: 0.4352\n",
      "Epoch 52/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.8251 - acc: 0.3357 - val_loss: 1.6110 - val_acc: 0.4408\n",
      "Epoch 53/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.8213 - acc: 0.3402 - val_loss: 1.5972 - val_acc: 0.4414\n",
      "Epoch 54/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.8023 - acc: 0.3458 - val_loss: 1.5879 - val_acc: 0.4438\n",
      "Epoch 55/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.8028 - acc: 0.3432 - val_loss: 1.5825 - val_acc: 0.4473\n",
      "Epoch 56/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.7942 - acc: 0.3471 - val_loss: 1.5815 - val_acc: 0.4517\n",
      "Epoch 57/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.7828 - acc: 0.3471 - val_loss: 1.5622 - val_acc: 0.4509\n",
      "Epoch 58/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.7712 - acc: 0.3492 - val_loss: 1.5524 - val_acc: 0.4498\n",
      "Epoch 59/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.7698 - acc: 0.3531 - val_loss: 1.5493 - val_acc: 0.4546\n",
      "Epoch 60/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.7603 - acc: 0.3585 - val_loss: 1.5451 - val_acc: 0.4561\n",
      "Epoch 61/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.7550 - acc: 0.3570 - val_loss: 1.5428 - val_acc: 0.4539\n",
      "Epoch 62/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.7423 - acc: 0.3625 - val_loss: 1.5336 - val_acc: 0.4577\n",
      "Epoch 63/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.7448 - acc: 0.3636 - val_loss: 1.5272 - val_acc: 0.4575\n",
      "Epoch 64/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.7330 - acc: 0.3708 - val_loss: 1.5195 - val_acc: 0.4579\n",
      "Epoch 65/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.7274 - acc: 0.3706 - val_loss: 1.5208 - val_acc: 0.4623\n",
      "Epoch 66/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.7250 - acc: 0.3698 - val_loss: 1.5133 - val_acc: 0.4596\n",
      "Epoch 67/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.7110 - acc: 0.3725 - val_loss: 1.5074 - val_acc: 0.4641\n",
      "Epoch 68/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.7080 - acc: 0.3800 - val_loss: 1.5052 - val_acc: 0.4644\n",
      "Epoch 69/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.7028 - acc: 0.3778 - val_loss: 1.4967 - val_acc: 0.4644\n",
      "Epoch 70/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.6972 - acc: 0.3814 - val_loss: 1.4917 - val_acc: 0.4697\n",
      "Epoch 71/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.6871 - acc: 0.3866 - val_loss: 1.4845 - val_acc: 0.4672\n",
      "Epoch 72/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.6878 - acc: 0.3865 - val_loss: 1.4919 - val_acc: 0.4693\n",
      "Epoch 73/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.6877 - acc: 0.3839 - val_loss: 1.4824 - val_acc: 0.4738\n",
      "Epoch 74/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.6804 - acc: 0.3841 - val_loss: 1.4804 - val_acc: 0.4717\n",
      "Epoch 75/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.6724 - acc: 0.3904 - val_loss: 1.4742 - val_acc: 0.4752\n",
      "Epoch 76/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.6639 - acc: 0.3962 - val_loss: 1.4727 - val_acc: 0.4768\n",
      "Epoch 77/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.6631 - acc: 0.3965 - val_loss: 1.4703 - val_acc: 0.4778\n",
      "Epoch 78/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.6538 - acc: 0.4001 - val_loss: 1.4645 - val_acc: 0.4727\n",
      "Epoch 79/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.6518 - acc: 0.3994 - val_loss: 1.4586 - val_acc: 0.4777\n",
      "Epoch 80/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.6477 - acc: 0.4036 - val_loss: 1.4625 - val_acc: 0.4825\n",
      "Epoch 81/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.6413 - acc: 0.4036 - val_loss: 1.4573 - val_acc: 0.4831\n",
      "Epoch 82/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.6388 - acc: 0.4070 - val_loss: 1.4508 - val_acc: 0.4833\n",
      "Epoch 83/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.6363 - acc: 0.4074 - val_loss: 1.4481 - val_acc: 0.4838\n",
      "Epoch 84/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.6265 - acc: 0.4083 - val_loss: 1.4459 - val_acc: 0.4821\n",
      "Epoch 85/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.6237 - acc: 0.4131 - val_loss: 1.4458 - val_acc: 0.4835\n",
      "Epoch 86/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.6189 - acc: 0.4155 - val_loss: 1.4389 - val_acc: 0.4825\n",
      "Epoch 87/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.6173 - acc: 0.4158 - val_loss: 1.4404 - val_acc: 0.4907\n",
      "Epoch 88/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.6097 - acc: 0.4189 - val_loss: 1.4330 - val_acc: 0.4870\n",
      "Epoch 89/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.6079 - acc: 0.4222 - val_loss: 1.4338 - val_acc: 0.4870\n",
      "Epoch 90/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.5981 - acc: 0.4224 - val_loss: 1.4272 - val_acc: 0.4905\n",
      "Epoch 91/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.5976 - acc: 0.4229 - val_loss: 1.4277 - val_acc: 0.4915\n",
      "Epoch 92/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.5828 - acc: 0.4241 - val_loss: 1.4239 - val_acc: 0.4940\n",
      "Epoch 93/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.5887 - acc: 0.4289 - val_loss: 1.4225 - val_acc: 0.4963\n",
      "Epoch 94/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.5865 - acc: 0.4262 - val_loss: 1.4186 - val_acc: 0.4912\n",
      "Epoch 95/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.5800 - acc: 0.4304 - val_loss: 1.4166 - val_acc: 0.4982\n",
      "Epoch 96/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.5793 - acc: 0.4322 - val_loss: 1.4179 - val_acc: 0.4969\n",
      "Epoch 97/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.5684 - acc: 0.4345 - val_loss: 1.4060 - val_acc: 0.4950\n",
      "Epoch 98/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.5715 - acc: 0.4324 - val_loss: 1.4089 - val_acc: 0.4967\n",
      "Epoch 99/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.5609 - acc: 0.4331 - val_loss: 1.4060 - val_acc: 0.4986\n",
      "Epoch 100/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.5649 - acc: 0.4354 - val_loss: 1.4049 - val_acc: 0.4998\n",
      "Epoch 101/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.5559 - acc: 0.4398 - val_loss: 1.4050 - val_acc: 0.4985\n",
      "Epoch 102/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.5571 - acc: 0.4403 - val_loss: 1.3994 - val_acc: 0.4975\n",
      "Epoch 103/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.5516 - acc: 0.4421 - val_loss: 1.3992 - val_acc: 0.4982\n",
      "Epoch 104/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.5466 - acc: 0.4458 - val_loss: 1.3999 - val_acc: 0.4974\n",
      "Epoch 105/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.5486 - acc: 0.4462 - val_loss: 1.3983 - val_acc: 0.5011\n",
      "Epoch 106/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.5412 - acc: 0.4470 - val_loss: 1.3932 - val_acc: 0.5012\n",
      "Epoch 107/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.5344 - acc: 0.4488 - val_loss: 1.3922 - val_acc: 0.5044\n",
      "Epoch 108/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.5282 - acc: 0.4518 - val_loss: 1.3900 - val_acc: 0.5013\n",
      "Epoch 109/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.5333 - acc: 0.4482 - val_loss: 1.3906 - val_acc: 0.5025\n",
      "Epoch 110/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.5307 - acc: 0.4509 - val_loss: 1.3853 - val_acc: 0.5053\n",
      "Epoch 111/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.5186 - acc: 0.4568 - val_loss: 1.3873 - val_acc: 0.5083\n",
      "Epoch 112/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.5238 - acc: 0.4541 - val_loss: 1.3842 - val_acc: 0.5071\n",
      "Epoch 113/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.5182 - acc: 0.4570 - val_loss: 1.3832 - val_acc: 0.5074\n",
      "Epoch 114/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.5189 - acc: 0.4600 - val_loss: 1.3881 - val_acc: 0.5088\n",
      "Epoch 115/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.5144 - acc: 0.4617 - val_loss: 1.3838 - val_acc: 0.5046\n",
      "Epoch 116/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.5064 - acc: 0.4615 - val_loss: 1.3773 - val_acc: 0.5084\n",
      "Epoch 117/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.5061 - acc: 0.4606 - val_loss: 1.3763 - val_acc: 0.5102\n",
      "Epoch 118/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.4998 - acc: 0.4669 - val_loss: 1.3728 - val_acc: 0.5150\n",
      "Epoch 119/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.5015 - acc: 0.4624 - val_loss: 1.3733 - val_acc: 0.5078\n",
      "Epoch 120/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.4953 - acc: 0.4688 - val_loss: 1.3748 - val_acc: 0.5120\n",
      "Epoch 121/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.4889 - acc: 0.4672 - val_loss: 1.3719 - val_acc: 0.5124\n",
      "Epoch 122/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.4867 - acc: 0.4649 - val_loss: 1.3729 - val_acc: 0.5159\n",
      "Epoch 123/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.4845 - acc: 0.4723 - val_loss: 1.3695 - val_acc: 0.5144\n",
      "Epoch 124/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.4871 - acc: 0.4706 - val_loss: 1.3681 - val_acc: 0.5149\n",
      "Epoch 125/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.4764 - acc: 0.4749 - val_loss: 1.3640 - val_acc: 0.5164\n",
      "Epoch 126/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.4821 - acc: 0.4723 - val_loss: 1.3638 - val_acc: 0.5169\n",
      "Epoch 127/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.4761 - acc: 0.4730 - val_loss: 1.3629 - val_acc: 0.5144\n",
      "Epoch 128/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.4725 - acc: 0.4760 - val_loss: 1.3630 - val_acc: 0.5186\n",
      "Epoch 129/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.4735 - acc: 0.4788 - val_loss: 1.3647 - val_acc: 0.5146\n",
      "Epoch 130/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.4655 - acc: 0.4781 - val_loss: 1.3596 - val_acc: 0.5186\n",
      "Epoch 131/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.4610 - acc: 0.4777 - val_loss: 1.3610 - val_acc: 0.5162\n",
      "Epoch 132/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.4623 - acc: 0.4787 - val_loss: 1.3597 - val_acc: 0.5181\n",
      "Epoch 133/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.4584 - acc: 0.4845 - val_loss: 1.3568 - val_acc: 0.5200\n",
      "Epoch 134/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.4515 - acc: 0.4830 - val_loss: 1.3552 - val_acc: 0.5231\n",
      "Epoch 135/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.4532 - acc: 0.4837 - val_loss: 1.3569 - val_acc: 0.5200\n",
      "Epoch 136/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.4552 - acc: 0.4861 - val_loss: 1.3547 - val_acc: 0.5230\n",
      "Epoch 137/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.4490 - acc: 0.4848 - val_loss: 1.3567 - val_acc: 0.5199\n",
      "Epoch 138/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.4473 - acc: 0.4853 - val_loss: 1.3527 - val_acc: 0.5232\n",
      "Epoch 139/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.4329 - acc: 0.4913 - val_loss: 1.3541 - val_acc: 0.5138\n",
      "Epoch 140/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.4484 - acc: 0.4853 - val_loss: 1.3532 - val_acc: 0.5255\n",
      "Epoch 141/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.4416 - acc: 0.4896 - val_loss: 1.3531 - val_acc: 0.5218\n",
      "Epoch 142/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.4380 - acc: 0.4884 - val_loss: 1.3501 - val_acc: 0.5170\n",
      "Epoch 143/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.4325 - acc: 0.4944 - val_loss: 1.3492 - val_acc: 0.5213\n",
      "Epoch 144/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.4317 - acc: 0.4937 - val_loss: 1.3501 - val_acc: 0.5202\n",
      "Epoch 145/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.4292 - acc: 0.4948 - val_loss: 1.3528 - val_acc: 0.5185\n",
      "Epoch 146/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.4276 - acc: 0.4937 - val_loss: 1.3491 - val_acc: 0.5164\n",
      "Epoch 147/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.4250 - acc: 0.4943 - val_loss: 1.3472 - val_acc: 0.5182\n",
      "Epoch 148/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.4233 - acc: 0.4950 - val_loss: 1.3462 - val_acc: 0.5246\n",
      "Epoch 149/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.4148 - acc: 0.4979 - val_loss: 1.3460 - val_acc: 0.5212\n",
      "Epoch 150/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.4185 - acc: 0.5009 - val_loss: 1.3464 - val_acc: 0.5250\n",
      "Epoch 151/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.4144 - acc: 0.5004 - val_loss: 1.3475 - val_acc: 0.5231\n",
      "Epoch 152/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.4166 - acc: 0.4946 - val_loss: 1.3455 - val_acc: 0.5227\n",
      "Epoch 153/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.4077 - acc: 0.5010 - val_loss: 1.3445 - val_acc: 0.5215\n",
      "Epoch 154/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.4120 - acc: 0.5002 - val_loss: 1.3427 - val_acc: 0.5245\n",
      "Epoch 155/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.4113 - acc: 0.5028 - val_loss: 1.3470 - val_acc: 0.5244\n",
      "Epoch 156/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.4020 - acc: 0.5025 - val_loss: 1.3443 - val_acc: 0.5248\n",
      "Epoch 157/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.4011 - acc: 0.5049 - val_loss: 1.3435 - val_acc: 0.5234\n",
      "Epoch 158/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3948 - acc: 0.5031 - val_loss: 1.3446 - val_acc: 0.5244\n",
      "Epoch 159/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3980 - acc: 0.5070 - val_loss: 1.3415 - val_acc: 0.5251\n",
      "Epoch 160/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3928 - acc: 0.5076 - val_loss: 1.3411 - val_acc: 0.5224\n",
      "Epoch 161/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3945 - acc: 0.5047 - val_loss: 1.3492 - val_acc: 0.5194\n",
      "Epoch 162/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3928 - acc: 0.5102 - val_loss: 1.3403 - val_acc: 0.5253\n",
      "Epoch 163/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3872 - acc: 0.5079 - val_loss: 1.3466 - val_acc: 0.5250\n",
      "Epoch 164/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3850 - acc: 0.5093 - val_loss: 1.3439 - val_acc: 0.5227\n",
      "Epoch 165/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3856 - acc: 0.5101 - val_loss: 1.3380 - val_acc: 0.5305\n",
      "Epoch 166/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3814 - acc: 0.5107 - val_loss: 1.3439 - val_acc: 0.5232\n",
      "Epoch 167/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3813 - acc: 0.5156 - val_loss: 1.3386 - val_acc: 0.5276\n",
      "Epoch 168/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3723 - acc: 0.5184 - val_loss: 1.3398 - val_acc: 0.5256\n",
      "Epoch 169/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3709 - acc: 0.5149 - val_loss: 1.3385 - val_acc: 0.5288\n",
      "Epoch 170/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3718 - acc: 0.5135 - val_loss: 1.3389 - val_acc: 0.5245\n",
      "Epoch 171/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3751 - acc: 0.5145 - val_loss: 1.3392 - val_acc: 0.5269\n",
      "Epoch 172/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3643 - acc: 0.5172 - val_loss: 1.3391 - val_acc: 0.5270\n",
      "Epoch 173/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3668 - acc: 0.5158 - val_loss: 1.3414 - val_acc: 0.5269\n",
      "Epoch 174/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3617 - acc: 0.5203 - val_loss: 1.3371 - val_acc: 0.5281\n",
      "Epoch 175/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3575 - acc: 0.5193 - val_loss: 1.3362 - val_acc: 0.5294\n",
      "Epoch 176/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3549 - acc: 0.5207 - val_loss: 1.3376 - val_acc: 0.5316\n",
      "Epoch 177/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3586 - acc: 0.5223 - val_loss: 1.3358 - val_acc: 0.5294\n",
      "Epoch 178/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3509 - acc: 0.5231 - val_loss: 1.3359 - val_acc: 0.5304\n",
      "Epoch 179/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3530 - acc: 0.5209 - val_loss: 1.3375 - val_acc: 0.5314\n",
      "Epoch 180/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3473 - acc: 0.5228 - val_loss: 1.3346 - val_acc: 0.5318\n",
      "Epoch 181/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3545 - acc: 0.5217 - val_loss: 1.3330 - val_acc: 0.5344\n",
      "Epoch 182/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3513 - acc: 0.5282 - val_loss: 1.3332 - val_acc: 0.5312\n",
      "Epoch 183/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3431 - acc: 0.5253 - val_loss: 1.3380 - val_acc: 0.5351\n",
      "Epoch 184/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3440 - acc: 0.5283 - val_loss: 1.3341 - val_acc: 0.5307\n",
      "Epoch 185/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3383 - acc: 0.5294 - val_loss: 1.3307 - val_acc: 0.5324\n",
      "Epoch 186/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3455 - acc: 0.5269 - val_loss: 1.3355 - val_acc: 0.5327\n",
      "Epoch 187/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3381 - acc: 0.5261 - val_loss: 1.3384 - val_acc: 0.5355\n",
      "Epoch 188/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3349 - acc: 0.5322 - val_loss: 1.3342 - val_acc: 0.5361\n",
      "Epoch 189/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3330 - acc: 0.5285 - val_loss: 1.3344 - val_acc: 0.5341\n",
      "Epoch 190/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3332 - acc: 0.5263 - val_loss: 1.3336 - val_acc: 0.5368\n",
      "Epoch 191/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3330 - acc: 0.5307 - val_loss: 1.3311 - val_acc: 0.5352\n",
      "Epoch 192/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3282 - acc: 0.5360 - val_loss: 1.3318 - val_acc: 0.5338\n",
      "Epoch 193/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3194 - acc: 0.5348 - val_loss: 1.3292 - val_acc: 0.5345\n",
      "Epoch 194/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3213 - acc: 0.5340 - val_loss: 1.3332 - val_acc: 0.5356\n",
      "Epoch 195/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3218 - acc: 0.5311 - val_loss: 1.3361 - val_acc: 0.5327\n",
      "Epoch 196/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3191 - acc: 0.5367 - val_loss: 1.3370 - val_acc: 0.5309\n",
      "Epoch 197/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3186 - acc: 0.5384 - val_loss: 1.3394 - val_acc: 0.5378\n",
      "Epoch 198/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3107 - acc: 0.5390 - val_loss: 1.3346 - val_acc: 0.5322\n",
      "Epoch 199/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3152 - acc: 0.5386 - val_loss: 1.3330 - val_acc: 0.5369\n",
      "Epoch 200/200\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 1.3071 - acc: 0.5398 - val_loss: 1.3358 - val_acc: 0.5345\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, activation='relu', input_dim= 512))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model_hist = model.fit(train_features, train_labels,\n",
    "                    epochs=500,\n",
    "                    batch_size=1024,\n",
    "                    validation_data=(validation_features, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "HuZPtgmBoK5D",
    "outputId": "95948e55-e24c-4b24-f6e4-2f6d2fadce2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9572 - acc: 0.6640 - val_loss: 1.5403 - val_acc: 0.5604\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9533 - acc: 0.6656 - val_loss: 1.5452 - val_acc: 0.5592\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9615 - acc: 0.6663 - val_loss: 1.5337 - val_acc: 0.5592\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9562 - acc: 0.6665 - val_loss: 1.5369 - val_acc: 0.5622\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9589 - acc: 0.6665 - val_loss: 1.5239 - val_acc: 0.5614\n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9457 - acc: 0.6689 - val_loss: 1.5459 - val_acc: 0.5582\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9511 - acc: 0.6672 - val_loss: 1.5380 - val_acc: 0.5613\n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9498 - acc: 0.6668 - val_loss: 1.5405 - val_acc: 0.5586\n",
      "Epoch 9/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9438 - acc: 0.6679 - val_loss: 1.5563 - val_acc: 0.5618\n",
      "Epoch 10/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9551 - acc: 0.6643 - val_loss: 1.5463 - val_acc: 0.5601\n",
      "Epoch 11/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9455 - acc: 0.6719 - val_loss: 1.5358 - val_acc: 0.5586\n",
      "Epoch 12/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9506 - acc: 0.6671 - val_loss: 1.5546 - val_acc: 0.5613\n",
      "Epoch 13/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9520 - acc: 0.6665 - val_loss: 1.5484 - val_acc: 0.5614\n",
      "Epoch 14/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9546 - acc: 0.6661 - val_loss: 1.5314 - val_acc: 0.5626\n",
      "Epoch 15/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9456 - acc: 0.6711 - val_loss: 1.5478 - val_acc: 0.5666\n",
      "Epoch 16/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9446 - acc: 0.6737 - val_loss: 1.5478 - val_acc: 0.5635\n",
      "Epoch 17/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9562 - acc: 0.6650 - val_loss: 1.5479 - val_acc: 0.5639\n",
      "Epoch 18/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9431 - acc: 0.6702 - val_loss: 1.5537 - val_acc: 0.5637\n",
      "Epoch 19/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9428 - acc: 0.6718 - val_loss: 1.5515 - val_acc: 0.5620\n",
      "Epoch 20/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9320 - acc: 0.6721 - val_loss: 1.5593 - val_acc: 0.5628\n",
      "Epoch 21/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9444 - acc: 0.6708 - val_loss: 1.5555 - val_acc: 0.5638\n",
      "Epoch 22/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9389 - acc: 0.6716 - val_loss: 1.5601 - val_acc: 0.5620\n",
      "Epoch 23/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9370 - acc: 0.6716 - val_loss: 1.5636 - val_acc: 0.5599\n",
      "Epoch 24/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9351 - acc: 0.6723 - val_loss: 1.5564 - val_acc: 0.5632\n",
      "Epoch 25/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9311 - acc: 0.6727 - val_loss: 1.5603 - val_acc: 0.5650\n",
      "Epoch 26/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9390 - acc: 0.6721 - val_loss: 1.5683 - val_acc: 0.5626\n",
      "Epoch 27/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9339 - acc: 0.6708 - val_loss: 1.5493 - val_acc: 0.5627\n",
      "Epoch 28/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9369 - acc: 0.6736 - val_loss: 1.5630 - val_acc: 0.5662\n",
      "Epoch 29/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9406 - acc: 0.6688 - val_loss: 1.5603 - val_acc: 0.5614\n",
      "Epoch 30/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9368 - acc: 0.6750 - val_loss: 1.5636 - val_acc: 0.5637\n",
      "Epoch 31/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9309 - acc: 0.6725 - val_loss: 1.5598 - val_acc: 0.5647\n",
      "Epoch 32/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9355 - acc: 0.6727 - val_loss: 1.5649 - val_acc: 0.5648\n",
      "Epoch 33/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9306 - acc: 0.6717 - val_loss: 1.5640 - val_acc: 0.5641\n",
      "Epoch 34/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9323 - acc: 0.6735 - val_loss: 1.5731 - val_acc: 0.5650\n",
      "Epoch 35/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9349 - acc: 0.6707 - val_loss: 1.5636 - val_acc: 0.5609\n",
      "Epoch 36/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9320 - acc: 0.6727 - val_loss: 1.5707 - val_acc: 0.5615\n",
      "Epoch 37/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9323 - acc: 0.6755 - val_loss: 1.5755 - val_acc: 0.5649\n",
      "Epoch 38/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9292 - acc: 0.6729 - val_loss: 1.5763 - val_acc: 0.5664\n",
      "Epoch 39/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9328 - acc: 0.6736 - val_loss: 1.5808 - val_acc: 0.5602\n",
      "Epoch 40/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9313 - acc: 0.6770 - val_loss: 1.5722 - val_acc: 0.5619\n",
      "Epoch 41/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9303 - acc: 0.6732 - val_loss: 1.5766 - val_acc: 0.5635\n",
      "Epoch 42/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9257 - acc: 0.6802 - val_loss: 1.5737 - val_acc: 0.5633\n",
      "Epoch 43/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9202 - acc: 0.6791 - val_loss: 1.5884 - val_acc: 0.5612\n",
      "Epoch 44/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9296 - acc: 0.6739 - val_loss: 1.5646 - val_acc: 0.5617\n",
      "Epoch 45/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9175 - acc: 0.6785 - val_loss: 1.5876 - val_acc: 0.5637\n",
      "Epoch 46/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9253 - acc: 0.6775 - val_loss: 1.5783 - val_acc: 0.5623\n",
      "Epoch 47/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9269 - acc: 0.6751 - val_loss: 1.5807 - val_acc: 0.5629\n",
      "Epoch 48/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9246 - acc: 0.6778 - val_loss: 1.5752 - val_acc: 0.5640\n",
      "Epoch 49/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9235 - acc: 0.6795 - val_loss: 1.5846 - val_acc: 0.5616\n",
      "Epoch 50/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9247 - acc: 0.6772 - val_loss: 1.5740 - val_acc: 0.5624\n",
      "Epoch 51/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9221 - acc: 0.6758 - val_loss: 1.5891 - val_acc: 0.5644\n",
      "Epoch 52/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9180 - acc: 0.6789 - val_loss: 1.5848 - val_acc: 0.5648\n",
      "Epoch 53/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9181 - acc: 0.6791 - val_loss: 1.5818 - val_acc: 0.5600\n",
      "Epoch 54/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9277 - acc: 0.6752 - val_loss: 1.5882 - val_acc: 0.5657\n",
      "Epoch 55/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9195 - acc: 0.6770 - val_loss: 1.5924 - val_acc: 0.5625\n",
      "Epoch 56/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9174 - acc: 0.6772 - val_loss: 1.5855 - val_acc: 0.5633\n",
      "Epoch 57/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9221 - acc: 0.6798 - val_loss: 1.5715 - val_acc: 0.5626\n",
      "Epoch 58/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9205 - acc: 0.6774 - val_loss: 1.5885 - val_acc: 0.5586\n",
      "Epoch 59/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9193 - acc: 0.6799 - val_loss: 1.5812 - val_acc: 0.5624\n",
      "Epoch 60/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9173 - acc: 0.6811 - val_loss: 1.5809 - val_acc: 0.5641\n",
      "Epoch 61/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9192 - acc: 0.6782 - val_loss: 1.5863 - val_acc: 0.5619\n",
      "Epoch 62/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9177 - acc: 0.6798 - val_loss: 1.5855 - val_acc: 0.5641\n",
      "Epoch 63/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9197 - acc: 0.6754 - val_loss: 1.5897 - val_acc: 0.5648\n",
      "Epoch 64/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9169 - acc: 0.6815 - val_loss: 1.5878 - val_acc: 0.5615\n",
      "Epoch 65/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9076 - acc: 0.6832 - val_loss: 1.5966 - val_acc: 0.5634\n",
      "Epoch 66/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9079 - acc: 0.6837 - val_loss: 1.6022 - val_acc: 0.5622\n",
      "Epoch 67/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9088 - acc: 0.6820 - val_loss: 1.6003 - val_acc: 0.5628\n",
      "Epoch 68/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9060 - acc: 0.6817 - val_loss: 1.6066 - val_acc: 0.5654\n",
      "Epoch 69/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9089 - acc: 0.6838 - val_loss: 1.6125 - val_acc: 0.5630\n",
      "Epoch 70/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9109 - acc: 0.6834 - val_loss: 1.6083 - val_acc: 0.5614\n",
      "Epoch 71/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9009 - acc: 0.6832 - val_loss: 1.6262 - val_acc: 0.5633\n",
      "Epoch 72/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9126 - acc: 0.6794 - val_loss: 1.6081 - val_acc: 0.5636\n",
      "Epoch 73/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9109 - acc: 0.6841 - val_loss: 1.6140 - val_acc: 0.5614\n",
      "Epoch 74/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9086 - acc: 0.6827 - val_loss: 1.6194 - val_acc: 0.5630\n",
      "Epoch 75/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9026 - acc: 0.6840 - val_loss: 1.6146 - val_acc: 0.5619\n",
      "Epoch 76/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9023 - acc: 0.6863 - val_loss: 1.6032 - val_acc: 0.5630\n",
      "Epoch 77/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.8949 - acc: 0.6872 - val_loss: 1.6144 - val_acc: 0.5612\n",
      "Epoch 78/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9016 - acc: 0.6836 - val_loss: 1.6082 - val_acc: 0.5637\n",
      "Epoch 79/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9020 - acc: 0.6854 - val_loss: 1.6149 - val_acc: 0.5638\n",
      "Epoch 80/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.8943 - acc: 0.6871 - val_loss: 1.6136 - val_acc: 0.5668\n",
      "Epoch 81/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.8935 - acc: 0.6848 - val_loss: 1.6132 - val_acc: 0.5657\n",
      "Epoch 82/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9021 - acc: 0.6875 - val_loss: 1.6091 - val_acc: 0.5629\n",
      "Epoch 83/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9023 - acc: 0.6836 - val_loss: 1.6235 - val_acc: 0.5632\n",
      "Epoch 84/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9021 - acc: 0.6835 - val_loss: 1.6132 - val_acc: 0.5665\n",
      "Epoch 85/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9030 - acc: 0.6854 - val_loss: 1.6126 - val_acc: 0.5628\n",
      "Epoch 86/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9003 - acc: 0.6856 - val_loss: 1.6094 - val_acc: 0.5660\n",
      "Epoch 87/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.8980 - acc: 0.6878 - val_loss: 1.6299 - val_acc: 0.5617\n",
      "Epoch 88/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9003 - acc: 0.6857 - val_loss: 1.6238 - val_acc: 0.5636\n",
      "Epoch 89/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.8925 - acc: 0.6876 - val_loss: 1.5976 - val_acc: 0.5639\n",
      "Epoch 90/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.8928 - acc: 0.6891 - val_loss: 1.6232 - val_acc: 0.5649\n",
      "Epoch 91/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.8990 - acc: 0.6884 - val_loss: 1.6280 - val_acc: 0.5658\n",
      "Epoch 92/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.8980 - acc: 0.6855 - val_loss: 1.6122 - val_acc: 0.5626\n",
      "Epoch 93/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.8975 - acc: 0.6862 - val_loss: 1.6086 - val_acc: 0.5630\n",
      "Epoch 94/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.8916 - acc: 0.6867 - val_loss: 1.6248 - val_acc: 0.5626\n",
      "Epoch 95/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.8910 - acc: 0.6898 - val_loss: 1.6290 - val_acc: 0.5618\n",
      "Epoch 96/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.9008 - acc: 0.6862 - val_loss: 1.6433 - val_acc: 0.5657\n",
      "Epoch 97/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.8849 - acc: 0.6883 - val_loss: 1.6175 - val_acc: 0.5659\n",
      "Epoch 98/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.8899 - acc: 0.6913 - val_loss: 1.6383 - val_acc: 0.5610\n",
      "Epoch 99/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.8912 - acc: 0.6906 - val_loss: 1.6361 - val_acc: 0.5630\n",
      "Epoch 100/100\n",
      "40000/40000 [==============================] - 0s 6us/sample - loss: 0.8885 - acc: 0.6899 - val_loss: 1.6332 - val_acc: 0.5651\n"
     ]
    }
   ],
   "source": [
    "model_hist = model.fit(train_features, train_labels,\n",
    "                    epochs=100,\n",
    "                    batch_size=1024,\n",
    "                    validation_data=(validation_features, validation_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "id": "drE_Bd91oUdE",
    "outputId": "7a88a819-23e4-4566-9a05-f31465edef43"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "train_loss = model_hist.history['loss']\n",
    "val_loss= model_hist.history['val_loss']\n",
    "\n",
    "epochs_val = range(0, len(train_loss))\n",
    "epochs = range(0, len(train_loss))\n",
    "\n",
    "plt.plot(epochs_val, val_loss, 'b+', label='Validation', c = 'purple')\n",
    "plt.plot(epochs, train_loss, 'bo', label='training', c ='green')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "x8lXYDZKotkD",
    "outputId": "f48abe52-5ba2-428d-e11e-ea4040800629"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 52us/sample - loss: 1.6048 - acc: 0.5664\n",
      "test_acc: 0.5664\n",
      "test loss:  1.6048228435516358\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_features, test_labels)\n",
    "print('test_acc:', test_acc)\n",
    "print(\"test loss: \", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UQW9xRBeprLf"
   },
   "outputs": [],
   "source": [
    "model.save_weights('cifar10_feature_extraction.h5', overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FeaturesExtraction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
